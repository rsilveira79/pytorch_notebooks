{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Using TensorFlow backend.\n",
      "/Users/rsilvei/Envs/nlp/lib/python3.6/importlib/_bootstrap.py:219: RuntimeWarning: compiletime version 3.5 of module 'tensorflow.python.framework.fast_tensor_util' does not match runtime version 3.6\n",
      "  return f(*args, **kwds)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[nltk_data] Error loading stopwords: <urlopen error [SSL:\n",
      "[nltk_data]     CERTIFICATE_VERIFY_FAILED] certificate verify failed\n",
      "[nltk_data]     (_ssl.c:777)>\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "False"
      ]
     },
     "execution_count": 1,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import re\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "\n",
    "## Pytorch Imports\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "from torch.autograd import Variable\n",
    "import torch.utils.data\n",
    "import torch.nn.functional as F\n",
    "import torch.optim as optim\n",
    "\n",
    "## NLP Libraries\n",
    "import spacy\n",
    "from sklearn.model_selection import train_test_split\n",
    "from nltk import download\n",
    "import gensim\n",
    "from nltk.corpus import stopwords\n",
    "\n",
    "spacy_en = spacy.load('en')\n",
    "download('stopwords')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 2. Dataset\n",
    "- more information here: https://www.kaggle.com/c/sentiment-analysis-on-movie-reviews\n",
    "- 0 - negative    \n",
    "- 1 - positive  "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style>\n",
       "    .dataframe thead tr:only-child th {\n",
       "        text-align: right;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: left;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>phrase</th>\n",
       "      <th>sentiment</th>\n",
       "      <th>sentiment_simple</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>A series of escapades demonstrating the adage ...</td>\n",
       "      <td>1</td>\n",
       "      <td>NEG</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>This quiet , introspective and entertaining in...</td>\n",
       "      <td>4</td>\n",
       "      <td>POS</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>Even fans of Ismail Merchant 's work , I suspe...</td>\n",
       "      <td>1</td>\n",
       "      <td>NEG</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>A positively thrilling combination of ethnogra...</td>\n",
       "      <td>3</td>\n",
       "      <td>POS</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>Aggressive self-glorification and a manipulati...</td>\n",
       "      <td>1</td>\n",
       "      <td>NEG</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                                              phrase sentiment  \\\n",
       "0  A series of escapades demonstrating the adage ...         1   \n",
       "1  This quiet , introspective and entertaining in...         4   \n",
       "2  Even fans of Ismail Merchant 's work , I suspe...         1   \n",
       "3  A positively thrilling combination of ethnogra...         3   \n",
       "4  Aggressive self-glorification and a manipulati...         1   \n",
       "\n",
       "  sentiment_simple  \n",
       "0              NEG  \n",
       "1              POS  \n",
       "2              NEG  \n",
       "3              POS  \n",
       "4              NEG  "
      ]
     },
     "execution_count": 2,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "train = pd.read_pickle('train.pkl')\n",
    "train.drop(labels='phrase_preprocessed', inplace=True,axis=1)\n",
    "train.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style>\n",
       "    .dataframe thead tr:only-child th {\n",
       "        text-align: right;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: left;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>phrase</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>An intermittently pleasing but mostly routine ...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>Kidman is really the only thing that 's worth ...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>Once you get into its rhythm ... the movie bec...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>I kept wishing I was watching a documentary ab...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>Kinnear does n't aim for our sympathy , but ra...</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                                              phrase\n",
       "0  An intermittently pleasing but mostly routine ...\n",
       "1  Kidman is really the only thing that 's worth ...\n",
       "2  Once you get into its rhythm ... the movie bec...\n",
       "3  I kept wishing I was watching a documentary ab...\n",
       "4  Kinnear does n't aim for our sympathy , but ra..."
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "test = pd.read_pickle('test.pkl')\n",
    "test.drop(labels='phrase_preprocessed', inplace=True,axis=1)\n",
    "test.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 3. Preprocessing"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "def transformText(text, do_stop=False, do_stem=False):\n",
    "    \n",
    "    stops = set(stopwords.words(\"english\"))\n",
    "    \n",
    "    # Convert text to lower\n",
    "    text = text.lower()\n",
    "    \n",
    "    # Removing non ASCII chars    \n",
    "    text = re.sub(r'[^\\x00-\\x7f]',r' ',text)\n",
    "    \n",
    "    # Strip multiple whitespaces\n",
    "    text = gensim.corpora.textcorpus.strip_multiple_whitespaces(text)\n",
    "    \n",
    "    # Removing all the stopwords\n",
    "    \n",
    "    if (do_stop==True):\n",
    "        filtered_words = [word for word in text.split() if word not in stops]\n",
    "    else:\n",
    "        filtered_words = [word for word in text.split()]\n",
    "\n",
    "    # Removing all the tokens with lesser than 3 characters\n",
    "    filtered_words = gensim.corpora.textcorpus.remove_short(filtered_words, minsize=2)\n",
    "    \n",
    "    # Preprocessed text after stop words removal\n",
    "    text = \" \".join(filtered_words)\n",
    "    \n",
    "    # Remove the punctuation\n",
    "    text = gensim.parsing.preprocessing.strip_punctuation2(text)\n",
    "    \n",
    "    # Strip all the numerics\n",
    "    text = gensim.parsing.preprocessing.strip_numeric(text)\n",
    "    \n",
    "    # Strip multiple whitespaces\n",
    "    text = gensim.corpora.textcorpus.strip_multiple_whitespaces(text)\n",
    "    \n",
    "    if (do_stem==True):\n",
    "        # Stemming\n",
    "        text = gensim.parsing.preprocessing.stem_text(text)\n",
    "    return text"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style>\n",
       "    .dataframe thead tr:only-child th {\n",
       "        text-align: right;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: left;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>phrase</th>\n",
       "      <th>sentiment</th>\n",
       "      <th>sentiment_simple</th>\n",
       "      <th>phrase_preprocessed</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>A series of escapades demonstrating the adage ...</td>\n",
       "      <td>1</td>\n",
       "      <td>NEG</td>\n",
       "      <td>series of escapades demonstrating the adage th...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>This quiet , introspective and entertaining in...</td>\n",
       "      <td>4</td>\n",
       "      <td>POS</td>\n",
       "      <td>this quiet introspective and entertaining inde...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>Even fans of Ismail Merchant 's work , I suspe...</td>\n",
       "      <td>1</td>\n",
       "      <td>NEG</td>\n",
       "      <td>even fans of ismail merchant s work suspect wo...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>A positively thrilling combination of ethnogra...</td>\n",
       "      <td>3</td>\n",
       "      <td>POS</td>\n",
       "      <td>positively thrilling combination of ethnograph...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>Aggressive self-glorification and a manipulati...</td>\n",
       "      <td>1</td>\n",
       "      <td>NEG</td>\n",
       "      <td>aggressive self glorification and manipulative...</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                                              phrase sentiment  \\\n",
       "0  A series of escapades demonstrating the adage ...         1   \n",
       "1  This quiet , introspective and entertaining in...         4   \n",
       "2  Even fans of Ismail Merchant 's work , I suspe...         1   \n",
       "3  A positively thrilling combination of ethnogra...         3   \n",
       "4  Aggressive self-glorification and a manipulati...         1   \n",
       "\n",
       "  sentiment_simple                                phrase_preprocessed  \n",
       "0              NEG  series of escapades demonstrating the adage th...  \n",
       "1              POS  this quiet introspective and entertaining inde...  \n",
       "2              NEG  even fans of ismail merchant s work suspect wo...  \n",
       "3              POS  positively thrilling combination of ethnograph...  \n",
       "4              NEG  aggressive self glorification and manipulative...  "
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "train['phrase_preprocessed']=train['phrase'].apply(lambda x: transformText(x,do_stop=False, do_stem=False))\n",
    "train.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "pandas.core.series.Series"
      ]
     },
     "execution_count": 9,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "a=train['phrase_preprocessed'].isnull()\n",
    "type(a)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "4927"
      ]
     },
     "execution_count": 11,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "len(train[train['sentiment_simple']=='NEG'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "3602"
      ]
     },
     "execution_count": 12,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "len(train[train['sentiment_simple']=='POS'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "= "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "Series([], Name: phrase_preprocessed, dtype: bool)"
      ]
     },
     "execution_count": 7,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "a[a==True]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style>\n",
       "    .dataframe thead tr:only-child th {\n",
       "        text-align: right;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: left;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>phrase</th>\n",
       "      <th>phrase_preprocessed</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>An intermittently pleasing but mostly routine ...</td>\n",
       "      <td>an intermittently pleasing but mostly routine ...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>Kidman is really the only thing that 's worth ...</td>\n",
       "      <td>kidman is really the only thing that s worth w...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>Once you get into its rhythm ... the movie bec...</td>\n",
       "      <td>once you get into its rhythm the movie becomes...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>I kept wishing I was watching a documentary ab...</td>\n",
       "      <td>kept wishing was watching documentary about th...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>Kinnear does n't aim for our sympathy , but ra...</td>\n",
       "      <td>kinnear does n t aim for our sympathy but rath...</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                                              phrase  \\\n",
       "0  An intermittently pleasing but mostly routine ...   \n",
       "1  Kidman is really the only thing that 's worth ...   \n",
       "2  Once you get into its rhythm ... the movie bec...   \n",
       "3  I kept wishing I was watching a documentary ab...   \n",
       "4  Kinnear does n't aim for our sympathy , but ra...   \n",
       "\n",
       "                                 phrase_preprocessed  \n",
       "0  an intermittently pleasing but mostly routine ...  \n",
       "1  kidman is really the only thing that s worth w...  \n",
       "2  once you get into its rhythm the movie becomes...  \n",
       "3  kept wishing was watching documentary about th...  \n",
       "4  kinnear does n t aim for our sympathy but rath...  "
      ]
     },
     "execution_count": 8,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "test['phrase_preprocessed']=test['phrase'].apply(lambda x: transformText(x,do_stop=False, do_stem=False))\n",
    "test.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 4. Train/Test split, Vocab"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "test_size = 0.2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "x_train, x_valid, y_train, y_valid = train_test_split(train['phrase_preprocessed'],\n",
    "                                                      train['sentiment_simple'], \n",
    "                                                      test_size=0.2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0    an intermittently pleasing but mostly routine ...\n",
       "1    kidman is really the only thing that s worth w...\n",
       "2    once you get into its rhythm the movie becomes...\n",
       "3    kept wishing was watching documentary about th...\n",
       "4    kinnear does n t aim for our sympathy but rath...\n",
       "Name: phrase_preprocessed, dtype: object"
      ]
     },
     "execution_count": 11,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "x_test=test['phrase_preprocessed']\n",
    "x_test[0:5]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "## Build Vocabulary\n",
    "word_to_ix = {}\n",
    "for sent in list(x_train) + list(x_valid) + list(x_test):\n",
    "    for word in sent.split():\n",
    "        if word not in word_to_ix:\n",
    "            word_to_ix[word] = len(word_to_ix)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'feeble': 0,\n",
       " 'comedy': 1,\n",
       " 'no': 2,\n",
       " 'doubt': 3,\n",
       " 'the': 4,\n",
       " 'star': 5,\n",
       " 'and': 6,\n",
       " 'everyone': 7,\n",
       " 'else': 8,\n",
       " 'involved': 9,\n",
       " 'had': 10,\n",
       " 'their': 11,\n",
       " 'hearts': 12,\n",
       " 'in': 13,\n",
       " 'right': 14,\n",
       " 'place': 15,\n",
       " 'you': 16,\n",
       " 'can': 17,\n",
       " 'drive': 18,\n",
       " 'by': 19,\n",
       " 'it': 20,\n",
       " 'without': 21,\n",
       " 'noticing': 22,\n",
       " 'anything': 23,\n",
       " 'special': 24,\n",
       " 'save': 25,\n",
       " 'for': 26,\n",
       " 'few': 27,\n",
       " 'comic': 28,\n",
       " 'turns': 29,\n",
       " 'intended': 30,\n",
       " 'otherwise': 31,\n",
       " 'an': 32,\n",
       " 'instance': 33,\n",
       " 'of': 34,\n",
       " 'old': 35,\n",
       " 'dog': 36,\n",
       " 'not': 37,\n",
       " 'only': 38,\n",
       " 'learning': 39,\n",
       " 'but': 40,\n",
       " 'inventing': 41,\n",
       " 'remarkable': 42,\n",
       " 'new': 43,\n",
       " 'trick': 44,\n",
       " 'although': 45,\n",
       " 'shot': 46,\n",
       " 'with': 47,\n",
       " 'little': 48,\n",
       " 'style': 49,\n",
       " 'skins': 50,\n",
       " 'is': 51,\n",
       " 'heartfelt': 52,\n",
       " 'achingly': 53,\n",
       " 'real': 54,\n",
       " 'fantastic': 55,\n",
       " 'premise': 56,\n",
       " 'anchors': 57,\n",
       " 'this': 58,\n",
       " 'movie': 59,\n",
       " 'what': 60,\n",
       " 'needs': 61,\n",
       " 'either': 62,\n",
       " 'more': 63,\n",
       " 'rigid': 64,\n",
       " 'blair': 65,\n",
       " 'witch': 66,\n",
       " 'commitment': 67,\n",
       " 'to': 68,\n",
       " 'its': 69,\n",
       " 'mockumentary': 70,\n",
       " 'format': 71,\n",
       " 'or': 72,\n",
       " 'straightforward': 73,\n",
       " 'dramatic': 74,\n",
       " 'treatment': 75,\n",
       " 'all': 76,\n",
       " 'grandiosity': 77,\n",
       " 'that': 78,\n",
       " 'implies': 79,\n",
       " 'wobbly': 80,\n",
       " 'senegalese': 81,\n",
       " 'updating': 82,\n",
       " 'carmen': 83,\n",
       " 'which': 84,\n",
       " 'best': 85,\n",
       " 'stunning': 86,\n",
       " 'turn': 87,\n",
       " 'djeinaba': 88,\n",
       " 'diop': 89,\n",
       " 'gai': 90,\n",
       " 'evokes': 91,\n",
       " 'flash': 92,\n",
       " 'double': 93,\n",
       " 'cross': 94,\n",
       " 'made': 95,\n",
       " 'mamet': 96,\n",
       " 's': 97,\n",
       " 'house': 98,\n",
       " 'games': 99,\n",
       " 'last': 100,\n",
       " 'fall': 101,\n",
       " 'heist': 102,\n",
       " 'so': 103,\n",
       " 'much': 104,\n",
       " 'fun': 105,\n",
       " 'film': 106,\n",
       " 'while': 107,\n",
       " 'exactly': 108,\n",
       " 'assured': 109,\n",
       " 'execution': 110,\n",
       " 'notable': 111,\n",
       " 'sheer': 112,\n",
       " 'audacity': 113,\n",
       " 'openness': 114,\n",
       " 'flat': 115,\n",
       " 'run': 116,\n",
       " 'at': 117,\n",
       " 'hip': 118,\n",
       " 'hop': 119,\n",
       " 'tootsie': 120,\n",
       " 'poorly': 121,\n",
       " 'paced': 122,\n",
       " 'could': 123,\n",
       " 'fit': 124,\n",
       " 'pootie': 125,\n",
       " 'tang': 126,\n",
       " 'between': 127,\n",
       " 'punchlines': 128,\n",
       " 'collapses': 129,\n",
       " 'when': 130,\n",
       " 'mr': 131,\n",
       " 'taylor': 132,\n",
       " 'tries': 133,\n",
       " 'shift': 134,\n",
       " 'tone': 135,\n",
       " 'thriller': 136,\n",
       " 'rush': 137,\n",
       " 'as': 138,\n",
       " 'tricky': 139,\n",
       " 'satisfying': 140,\n",
       " 'any': 141,\n",
       " 'david': 142,\n",
       " 'airless': 143,\n",
       " 'cinematic': 144,\n",
       " 'shell': 145,\n",
       " 'have': 146,\n",
       " 'been': 147,\n",
       " 'something': 148,\n",
       " 'two': 149,\n",
       " 'things': 150,\n",
       " 'drag': 151,\n",
       " 'down': 152,\n",
       " 'mediocrity': 153,\n",
       " 'director': 154,\n",
       " 'clare': 155,\n",
       " 'peploe': 156,\n",
       " 'misunderstanding': 157,\n",
       " 'marivaux': 158,\n",
       " 'rhythms': 159,\n",
       " 'mira': 160,\n",
       " 'sorvino': 161,\n",
       " 'limitations': 162,\n",
       " 'classical': 163,\n",
       " 'actress': 164,\n",
       " 'music': 165,\n",
       " 'makes': 166,\n",
       " 'nice': 167,\n",
       " 'album': 168,\n",
       " 'food': 169,\n",
       " 'enticing': 170,\n",
       " 'italy': 171,\n",
       " 'beckons': 172,\n",
       " 'us': 173,\n",
       " 'may': 174,\n",
       " 'be': 175,\n",
       " 'captivated': 176,\n",
       " 'was': 177,\n",
       " 'moods': 178,\n",
       " 'subtly': 179,\n",
       " 'transformed': 180,\n",
       " 'still': 181,\n",
       " 'wonder': 182,\n",
       " 'why': 183,\n",
       " 'paul': 184,\n",
       " 'thomas': 185,\n",
       " 'anderson': 186,\n",
       " 'ever': 187,\n",
       " 'inclination': 188,\n",
       " 'make': 189,\n",
       " 'most': 190,\n",
       " 'sincere': 191,\n",
       " 'artful': 192,\n",
       " 'adam': 193,\n",
       " 'sandler': 194,\n",
       " 'will': 195,\n",
       " 'probably': 196,\n",
       " 'appear': 197,\n",
       " 'remake': 198,\n",
       " 'pale': 199,\n",
       " 'imitation': 200,\n",
       " 'filmmakers': 201,\n",
       " 'dana': 202,\n",
       " 'janklowicz': 203,\n",
       " 'mann': 204,\n",
       " 'amir': 205,\n",
       " 'area': 206,\n",
       " 'headed': 207,\n",
       " 'east': 208,\n",
       " 'far': 209,\n",
       " 'retelling': 210,\n",
       " 'historically': 211,\n",
       " 'significant': 212,\n",
       " 'personal': 213,\n",
       " 'episode': 214,\n",
       " 'detailing': 215,\n",
       " 'how': 216,\n",
       " 'one': 217,\n",
       " 'international': 218,\n",
       " 'city': 219,\n",
       " 'welcomed': 220,\n",
       " 'tens': 221,\n",
       " 'thousands': 222,\n",
       " 'german': 223,\n",
       " 'jewish': 224,\n",
       " 'refugees': 225,\n",
       " 'world': 226,\n",
       " 'democracie': 227,\n",
       " 'bites': 228,\n",
       " 'hard': 229,\n",
       " 'troupe': 230,\n",
       " 'broken': 231,\n",
       " 'lizard': 232,\n",
       " 'first': 233,\n",
       " 'very': 234,\n",
       " 'funny': 235,\n",
       " 'too': 236,\n",
       " 'concerned': 237,\n",
       " 'giving': 238,\n",
       " 'plot': 239,\n",
       " 'shanghai': 240,\n",
       " 'ghetto': 241,\n",
       " 'roman': 242,\n",
       " 'polanski': 243,\n",
       " 'pianist': 244,\n",
       " 'compassionate': 245,\n",
       " 'spirit': 246,\n",
       " 'soars': 247,\n",
       " 'every': 248,\n",
       " 'bit': 249,\n",
       " 'high': 250,\n",
       " 'includes': 251,\n",
       " 'obvious': 252,\n",
       " 'padding': 253,\n",
       " 'both': 254,\n",
       " 'grant': 255,\n",
       " 'n': 256,\n",
       " 't': 257,\n",
       " 'cary': 258,\n",
       " 'bullock': 259,\n",
       " 'katherine': 260,\n",
       " 'benigni': 261,\n",
       " 'presents': 262,\n",
       " 'himself': 263,\n",
       " 'boy': 264,\n",
       " 'puppet': 265,\n",
       " 'pinocchio': 266,\n",
       " 'complete': 267,\n",
       " 'receding': 268,\n",
       " 'hairline': 269,\n",
       " 'weathered': 270,\n",
       " 'countenance': 271,\n",
       " 'american': 272,\n",
       " 'breckin': 273,\n",
       " 'meyer': 274,\n",
       " 'ridiculously': 275,\n",
       " 'inappropriate': 276,\n",
       " 'valley': 277,\n",
       " 'voice': 278,\n",
       " 'fresnadillo': 279,\n",
       " 'has': 280,\n",
       " 'serious': 281,\n",
       " 'say': 282,\n",
       " 'about': 283,\n",
       " 'ways': 284,\n",
       " 'extravagant': 285,\n",
       " 'chance': 286,\n",
       " 'distort': 287,\n",
       " 'our': 288,\n",
       " 'perspective': 289,\n",
       " 'throw': 290,\n",
       " 'off': 291,\n",
       " 'path': 292,\n",
       " 'good': 293,\n",
       " 'sense': 294,\n",
       " 'keenly': 295,\n",
       " 'observed': 296,\n",
       " 'refreshingly': 297,\n",
       " 'natural': 298,\n",
       " 'swimming': 299,\n",
       " 'gets': 300,\n",
       " 'details': 301,\n",
       " 'from': 302,\n",
       " 'promenade': 303,\n",
       " 'barely': 304,\n",
       " 'clad': 305,\n",
       " 'bodies': 306,\n",
       " 'myrtle': 307,\n",
       " 'beach': 308,\n",
       " 'c': 309,\n",
       " 'adrenaline': 310,\n",
       " 'jolt': 311,\n",
       " 'sudden': 312,\n",
       " 'lunch': 313,\n",
       " 'diner': 314,\n",
       " 'auteur': 315,\n",
       " 'ear': 316,\n",
       " 'way': 317,\n",
       " 'fears': 318,\n",
       " 'slights': 319,\n",
       " 'are': 320,\n",
       " 'telegraphed': 321,\n",
       " 'blithe': 322,\n",
       " 'exchanges': 323,\n",
       " 'gives': 324,\n",
       " 'lingering': 325,\n",
       " 'tug': 326,\n",
       " 'suffice': 327,\n",
       " 'after': 328,\n",
       " 'seeing': 329,\n",
       " 'imax': 330,\n",
       " 'form': 331,\n",
       " 'll': 332,\n",
       " 'acquainted': 333,\n",
       " 'tiniest': 334,\n",
       " 'tom': 335,\n",
       " 'hanks': 336,\n",
       " 'face': 337,\n",
       " 'than': 338,\n",
       " 'his': 339,\n",
       " 'wife': 340,\n",
       " 'dull': 341,\n",
       " 'somnambulant': 342,\n",
       " 'exercise': 343,\n",
       " 'pretension': 344,\n",
       " 'whose': 345,\n",
       " 'pervasive': 346,\n",
       " 'quiet': 347,\n",
       " 'frequent': 348,\n",
       " 'outbursts': 349,\n",
       " 'violence': 350,\n",
       " 'noise': 351,\n",
       " 'gangster': 352,\n",
       " 'solid': 353,\n",
       " 'fare': 354,\n",
       " 'adults': 355,\n",
       " 'efforts': 356,\n",
       " 'kline': 357,\n",
       " 'lend': 358,\n",
       " 'some': 359,\n",
       " 'dignity': 360,\n",
       " 'dumb': 361,\n",
       " 'story': 362,\n",
       " 'naught': 363,\n",
       " 'there': 364,\n",
       " 'movies': 365,\n",
       " 'hit': 366,\n",
       " 'scene': 367,\n",
       " 'know': 368,\n",
       " 'going': 369,\n",
       " 'trip': 370,\n",
       " 'scherfig': 371,\n",
       " 'light': 372,\n",
       " 'hearted': 373,\n",
       " 'profile': 374,\n",
       " 'emotional': 375,\n",
       " 'desperation': 376,\n",
       " 'honest': 377,\n",
       " 'delightfully': 378,\n",
       " 'cheeky': 379,\n",
       " 'mattei': 380,\n",
       " 'underdeveloped': 381,\n",
       " 'effort': 382,\n",
       " 'here': 383,\n",
       " 'nothing': 384,\n",
       " 'convenient': 385,\n",
       " 'conveyor': 386,\n",
       " 'belt': 387,\n",
       " 'brooding': 388,\n",
       " 'personalities': 389,\n",
       " 'parade': 390,\n",
       " 'if': 391,\n",
       " 'they': 392,\n",
       " 'were': 393,\n",
       " 'coming': 394,\n",
       " 'back': 395,\n",
       " 'stock': 396,\n",
       " 'character': 397,\n",
       " 'camp': 398,\n",
       " 'drowsy': 399,\n",
       " 'drama': 400,\n",
       " 'infatuated': 401,\n",
       " 'own': 402,\n",
       " 'pretentious': 403,\n",
       " 'self': 404,\n",
       " 'examination': 405,\n",
       " 'zings': 406,\n",
       " 'through': 407,\n",
       " 'originality': 408,\n",
       " 'humour': 409,\n",
       " 'pathos': 410,\n",
       " 'masterfully': 411,\n",
       " 'calibrated': 412,\n",
       " 'psychological': 413,\n",
       " 'thrives': 414,\n",
       " 'on': 415,\n",
       " 'taut': 416,\n",
       " 'performances': 417,\n",
       " 'creepy': 418,\n",
       " 'atmosphere': 419,\n",
       " 'even': 420,\n",
       " 'screenplay': 421,\n",
       " 'falls': 422,\n",
       " 'somewhat': 423,\n",
       " 'short': 424,\n",
       " 'charming': 425,\n",
       " 'clashing': 426,\n",
       " 'cultures': 427,\n",
       " 'mother': 428,\n",
       " 'daughter': 429,\n",
       " 'relationship': 430,\n",
       " 'moral': 431,\n",
       " 'shrapnel': 432,\n",
       " 'mental': 433,\n",
       " 'shellshock': 434,\n",
       " 'linger': 435,\n",
       " 'long': 436,\n",
       " 'ended': 437,\n",
       " 'incredibly': 438,\n",
       " 'thoughtful': 439,\n",
       " 'deeply': 440,\n",
       " 'meditative': 441,\n",
       " 'picture': 442,\n",
       " 'neatly': 443,\n",
       " 'effectively': 444,\n",
       " 'captures': 445,\n",
       " 'debilitating': 446,\n",
       " 'grief': 447,\n",
       " 'felt': 448,\n",
       " 'immediate': 449,\n",
       " 'aftermath': 450,\n",
       " 'terrorist': 451,\n",
       " 'attacks': 452,\n",
       " 'films': 453,\n",
       " 'year': 454,\n",
       " 'exquisite': 455,\n",
       " 'acting': 456,\n",
       " 'inventive': 457,\n",
       " 'mesmerizing': 458,\n",
       " 'many': 459,\n",
       " 'inimitable': 460,\n",
       " 'scenes': 461,\n",
       " 'tenderness': 462,\n",
       " 'loss': 463,\n",
       " 'discontent': 464,\n",
       " 'yearning': 465,\n",
       " 'baaaaaaaaad': 466,\n",
       " 'van': 467,\n",
       " 'wilder': 468,\n",
       " 'worst': 469,\n",
       " 'national': 470,\n",
       " 'lampoon': 471,\n",
       " 'being': 472,\n",
       " 'generation': 473,\n",
       " 'animal': 474,\n",
       " 'think': 475,\n",
       " 'twice': 476,\n",
       " 'might': 477,\n",
       " 'inside': 478,\n",
       " 'each': 479,\n",
       " 'trailer': 480,\n",
       " 'park': 481,\n",
       " 'past': 482,\n",
       " 'chiefly': 483,\n",
       " 'inspires': 484,\n",
       " 'faster': 485,\n",
       " 'enough': 486,\n",
       " 'gun': 487,\n",
       " 'battles': 488,\n",
       " 'throwaway': 489,\n",
       " 'humor': 490,\n",
       " 'cover': 491,\n",
       " 'up': 492,\n",
       " 'yawning': 493,\n",
       " 'chasm': 494,\n",
       " 'where': 495,\n",
       " 'should': 496,\n",
       " 'same': 497,\n",
       " 'mistake': 498,\n",
       " 'industry': 499,\n",
       " 'criticizes': 500,\n",
       " 'becoming': 501,\n",
       " 'slick': 502,\n",
       " 'watered': 503,\n",
       " 'almost': 504,\n",
       " 'loses': 505,\n",
       " 'love': 506,\n",
       " 'biggest': 507,\n",
       " 'problem': 508,\n",
       " 'lrb': 509,\n",
       " 'other': 510,\n",
       " 'sluggish': 511,\n",
       " 'pace': 512,\n",
       " 'rrb': 513,\n",
       " 'we': 514,\n",
       " 'never': 515,\n",
       " 'really': 516,\n",
       " 'see': 517,\n",
       " 'her': 518,\n",
       " 'esther': 519,\n",
       " 'blossom': 520,\n",
       " 'though': 521,\n",
       " 'talent': 522,\n",
       " 'supposed': 523,\n",
       " 'growing': 524,\n",
       " 'better': 525,\n",
       " 'understand': 526,\n",
       " 'did': 527,\n",
       " 'connect': 528,\n",
       " 'me': 529,\n",
       " 'would': 530,\n",
       " 'require': 531,\n",
       " 'another': 532,\n",
       " 'viewing': 533,\n",
       " 'wo': 534,\n",
       " 'sitting': 535,\n",
       " 'again': 536,\n",
       " 'itself': 537,\n",
       " 'commentary': 538,\n",
       " 'sheridan': 539,\n",
       " 'painfully': 540,\n",
       " 'bad': 541,\n",
       " 'fourth': 542,\n",
       " 'rate': 543,\n",
       " 'jim': 544,\n",
       " 'carrey': 545,\n",
       " 'who': 546,\n",
       " 'does': 547,\n",
       " 'difference': 548,\n",
       " 'just': 549,\n",
       " 'plain': 550,\n",
       " 'carvey': 551,\n",
       " 'characters': 552,\n",
       " 'overplayed': 553,\n",
       " 'exaggerated': 554,\n",
       " 'then': 555,\n",
       " 'subtlety': 556,\n",
       " 'trademark': 557,\n",
       " 'master': 558,\n",
       " 'disguise': 559,\n",
       " 'under': 560,\n",
       " 'category': 561,\n",
       " 'sketch': 562,\n",
       " 'saturday': 563,\n",
       " 'night': 564,\n",
       " 'live': 565,\n",
       " 'john': 566,\n",
       " 'carlen': 567,\n",
       " 'script': 568,\n",
       " 'full': 569,\n",
       " 'unhappy': 570,\n",
       " 'dimensional': 571,\n",
       " 'compelling': 572,\n",
       " 'small': 573,\n",
       " 'big': 574,\n",
       " 'heart': 575,\n",
       " 'age': 576,\n",
       " 'arduous': 577,\n",
       " 'journey': 578,\n",
       " 'sensitive': 579,\n",
       " 'young': 580,\n",
       " 'girl': 581,\n",
       " 'series': 582,\n",
       " 'foster': 583,\n",
       " 'homes': 584,\n",
       " 'fierce': 585,\n",
       " 'struggle': 586,\n",
       " 'pull': 587,\n",
       " 'free': 588,\n",
       " 'dangerous': 589,\n",
       " 'domineering': 590,\n",
       " 'hold': 591,\n",
       " 'over': 592,\n",
       " 'despite': 593,\n",
       " 'flaws': 594,\n",
       " 'crazy': 595,\n",
       " 'hell': 596,\n",
       " 'marks': 597,\n",
       " 'encouraging': 598,\n",
       " 'direction': 599,\n",
       " 'la': 600,\n",
       " 'salle': 601,\n",
       " 'shallow': 602,\n",
       " 'welcome': 603,\n",
       " 'accept': 604,\n",
       " 'trials': 605,\n",
       " 'henry': 606,\n",
       " 'kissinger': 607,\n",
       " 'faithful': 608,\n",
       " 'portraiture': 609,\n",
       " 'argue': 610,\n",
       " 'debate': 611,\n",
       " 'joins': 612,\n",
       " 'necessary': 613,\n",
       " 'timely': 614,\n",
       " 'apart': 615,\n",
       " 'considerable': 616,\n",
       " 'achievement': 617,\n",
       " 'goyer': 618,\n",
       " 'loose': 619,\n",
       " 'unaccountable': 620,\n",
       " 'technically': 621,\n",
       " 'sophisticated': 622,\n",
       " 'runs': 623,\n",
       " 'longer': 624,\n",
       " 'muccino': 625,\n",
       " 'notice': 626,\n",
       " 'ends': 627,\n",
       " 'ca': 628,\n",
       " 'tear': 629,\n",
       " 'away': 630,\n",
       " 'smooth': 631,\n",
       " 'professional': 632,\n",
       " 'tsai': 633,\n",
       " 'ploughing': 634,\n",
       " 'furrow': 635,\n",
       " 'once': 636,\n",
       " 'often': 637,\n",
       " 'end': 638,\n",
       " 'bogs': 639,\n",
       " 'insignificance': 640,\n",
       " 'saying': 641,\n",
       " 'kennedy': 642,\n",
       " 'assassination': 643,\n",
       " 'revealing': 644,\n",
       " 'pathology': 645,\n",
       " 'pretends': 646,\n",
       " 'investigate': 647,\n",
       " 'essentially': 648,\n",
       " 'fleetingly': 649,\n",
       " 'interesting': 650,\n",
       " 'actors': 651,\n",
       " 'moments': 652,\n",
       " 'passion': 653,\n",
       " 'irwin': 654,\n",
       " 'earnest': 655,\n",
       " 'resist': 656,\n",
       " 'pleas': 657,\n",
       " 'spare': 658,\n",
       " 'wildlife': 659,\n",
       " 'respect': 660,\n",
       " 'environs': 661,\n",
       " 'rarely': 662,\n",
       " 'indeed': 663,\n",
       " 'such': 664,\n",
       " 'wattage': 665,\n",
       " 'brainpower': 666,\n",
       " 'coupled': 667,\n",
       " 'pitch': 668,\n",
       " 'perfect': 669,\n",
       " 'unfakable': 670,\n",
       " 'cinema': 671,\n",
       " 'antwone': 672,\n",
       " 'fisher': 673,\n",
       " 'certainly': 674,\n",
       " 'making': 675,\n",
       " 'care': 676,\n",
       " 'protagonist': 677,\n",
       " 'celebrate': 678,\n",
       " 'victories': 679,\n",
       " 'exceptions': 680,\n",
       " 'stoops': 681,\n",
       " 'cheap': 682,\n",
       " 'manipulation': 683,\n",
       " 'corny': 684,\n",
       " 'conventions': 685,\n",
       " 'do': 686,\n",
       " 'intriguing': 687,\n",
       " 'twist': 688,\n",
       " 'french': 689,\n",
       " 'genre': 690,\n",
       " 'princess': 691,\n",
       " 'seem': 692,\n",
       " 'smug': 693,\n",
       " 'cartoonish': 694,\n",
       " 'comes': 695,\n",
       " 'alive': 696,\n",
       " 'poor': 697,\n",
       " 'hermocrates': 698,\n",
       " 'leontine': 699,\n",
       " 'pathetically': 700,\n",
       " 'compare': 701,\n",
       " 'notes': 702,\n",
       " 'budding': 703,\n",
       " 'amours': 704,\n",
       " 'aside': 705,\n",
       " 'fact': 706,\n",
       " 'idiotically': 707,\n",
       " 'uses': 708,\n",
       " 'website': 709,\n",
       " 'feardotcom': 710,\n",
       " 'com': 711,\n",
       " 'improperly': 712,\n",
       " 'hammy': 713,\n",
       " 'performance': 714,\n",
       " 'stephen': 715,\n",
       " 'rea': 716,\n",
       " 'added': 717,\n",
       " 'disdain': 718,\n",
       " 'nearly': 719,\n",
       " 'impossible': 720,\n",
       " 'look': 721,\n",
       " 'perfectly': 722,\n",
       " 'competent': 723,\n",
       " 'imaginative': 724,\n",
       " 'lacks': 725,\n",
       " 'lilo': 726,\n",
       " 'stitch': 727,\n",
       " 'spades': 728,\n",
       " 'charisma': 729,\n",
       " 'overwrought': 730,\n",
       " 'ending': 731,\n",
       " 'works': 732,\n",
       " 'well': 733,\n",
       " 'because': 734,\n",
       " 'people': 735,\n",
       " 'want': 736,\n",
       " 'ol': 737,\n",
       " 'ball': 738,\n",
       " 'chain': 739,\n",
       " 'those': 740,\n",
       " 'waterboy': 741,\n",
       " 'major': 742,\n",
       " 'windtalkers': 743,\n",
       " 'bulk': 744,\n",
       " 'centers': 745,\n",
       " 'wrong': 746,\n",
       " 'hilarity': 747,\n",
       " 'simply': 748,\n",
       " 'recommend': 749,\n",
       " 'determination': 750,\n",
       " 'pinochet': 751,\n",
       " 'victims': 752,\n",
       " 'seek': 753,\n",
       " 'justice': 754,\n",
       " 'heartbreaking': 755,\n",
       " 'testimony': 756,\n",
       " 'spoken': 757,\n",
       " 'directly': 758,\n",
       " 'into': 759,\n",
       " 'patricio': 760,\n",
       " 'guzman': 761,\n",
       " 'camera': 762,\n",
       " 'pack': 763,\n",
       " 'powerful': 764,\n",
       " 'wallop': 765,\n",
       " 'road': 766,\n",
       " 'tight': 767,\n",
       " 'pants': 768,\n",
       " 'tits': 769,\n",
       " 'stupid': 770,\n",
       " 'ramsay': 771,\n",
       " 'morton': 772,\n",
       " 'fill': 773,\n",
       " 'study': 774,\n",
       " 'poetic': 775,\n",
       " 'force': 776,\n",
       " 'buoyant': 777,\n",
       " 'feeling': 778,\n",
       " 'consider': 779,\n",
       " 'review': 780,\n",
       " 'life': 781,\n",
       " 'affirming': 782,\n",
       " 'turned': 783,\n",
       " 'out': 784,\n",
       " 'hours': 785,\n",
       " 'unfocused': 786,\n",
       " 'excruciatingly': 787,\n",
       " 'tedious': 788,\n",
       " 'half': 789,\n",
       " 'hour': 790,\n",
       " 'starts': 791,\n",
       " 'water': 792,\n",
       " 'torture': 793,\n",
       " 'appealing': 794,\n",
       " 'narc': 795,\n",
       " 'menace': 796,\n",
       " 'exception': 797,\n",
       " 'blighter': 798,\n",
       " 'particular': 799,\n",
       " 'south': 800,\n",
       " 'london': 801,\n",
       " 'housing': 802,\n",
       " 'project': 803,\n",
       " 'digs': 804,\n",
       " 'dysfunction': 805,\n",
       " 'like': 806,\n",
       " 'comforting': 807,\n",
       " 'jar': 808,\n",
       " 'marmite': 809,\n",
       " 'slathered': 810,\n",
       " 'crackers': 811,\n",
       " 'served': 812,\n",
       " 'feast': 813,\n",
       " 'bleakness': 814,\n",
       " 'instead': 815,\n",
       " 'go': 816,\n",
       " 'rent': 817,\n",
       " 'shakes': 818,\n",
       " 'clown': 819,\n",
       " 'funnier': 820,\n",
       " 'similar': 821,\n",
       " 'theme': 822,\n",
       " 'equally': 823,\n",
       " 'great': 824,\n",
       " 'robin': 825,\n",
       " 'williams': 826,\n",
       " 'spy': 827,\n",
       " 'kids': 828,\n",
       " 'island': 829,\n",
       " 'lost': 830,\n",
       " 'dreams': 831,\n",
       " 'franchise': 832,\n",
       " 'establishes': 833,\n",
       " 'durable': 834,\n",
       " 'part': 835,\n",
       " 'landscape': 836,\n",
       " 'james': 837,\n",
       " 'bond': 838,\n",
       " 'action': 839,\n",
       " 'icon': 840,\n",
       " 'decommissioned': 841,\n",
       " 'your': 842,\n",
       " 'subject': 843,\n",
       " 'illusion': 844,\n",
       " 'versus': 845,\n",
       " 'reality': 846,\n",
       " 'least': 847,\n",
       " 'passably': 848,\n",
       " 'testament': 849,\n",
       " 'divine': 850,\n",
       " 'calling': 851,\n",
       " 'education': 852,\n",
       " 'demonstration': 853,\n",
       " 'painstaking': 854,\n",
       " 'process': 855,\n",
       " 'imparting': 856,\n",
       " 'knowledge': 857,\n",
       " 'remarkably': 858,\n",
       " 'alluring': 859,\n",
       " 'set': 860,\n",
       " 'constrictive': 861,\n",
       " 'eisenhower': 862,\n",
       " 'era': 863,\n",
       " 'suburban': 864,\n",
       " 'woman': 865,\n",
       " 'shatters': 866,\n",
       " 'cheery': 867,\n",
       " 'tranquil': 868,\n",
       " 'averse': 869,\n",
       " 'usually': 870,\n",
       " 'am': 871,\n",
       " 'feel': 872,\n",
       " 'follow': 873,\n",
       " 'dream': 874,\n",
       " 'hollywood': 875,\n",
       " 'fantasies': 876,\n",
       " 'got': 877,\n",
       " 'concert': 878,\n",
       " 'footage': 879,\n",
       " 'stirring': 880,\n",
       " 'recording': 881,\n",
       " 'sessions': 882,\n",
       " 'striking': 883,\n",
       " 'blow': 884,\n",
       " 'artistic': 885,\n",
       " 'integrity': 886,\n",
       " 'quality': 887,\n",
       " 'band': 888,\n",
       " 'pick': 889,\n",
       " 'admirers': 890,\n",
       " 'rare': 891,\n",
       " 'documentary': 892,\n",
       " 'incorporates': 893,\n",
       " 'human': 894,\n",
       " 'experience': 895,\n",
       " 'conflict': 896,\n",
       " 'tears': 897,\n",
       " 'surprise': 898,\n",
       " 'transcends': 899,\n",
       " 'normal': 900,\n",
       " 'divisions': 901,\n",
       " 'fiction': 902,\n",
       " 'nonfiction': 903,\n",
       " 'payne': 904,\n",
       " 'created': 905,\n",
       " 'beautiful': 906,\n",
       " 'canvas': 907,\n",
       " 'nicholson': 908,\n",
       " 'proves': 909,\n",
       " 'he': 910,\n",
       " 'brush': 911,\n",
       " 'business': 912,\n",
       " 'robert': 913,\n",
       " 'burke': 914,\n",
       " 'monster': 915,\n",
       " 'horns': 916,\n",
       " 'steals': 917,\n",
       " 'show': 918,\n",
       " 'family': 919,\n",
       " 'sour': 920,\n",
       " 'immortals': 921,\n",
       " 'saved': 922,\n",
       " 'merely': 923,\n",
       " 'cool': 924,\n",
       " 'basic': 925,\n",
       " 'credible': 926,\n",
       " 'compassion': 927,\n",
       " 'reminiscent': 928,\n",
       " 'unforgiven': 929,\n",
       " 'also': 930,\n",
       " 'utilized': 931,\n",
       " 'scintillating': 932,\n",
       " 'draw': 933,\n",
       " 'sparse': 934,\n",
       " 'dialogue': 935,\n",
       " 'shamelessly': 936,\n",
       " 'resorting': 937,\n",
       " 'pee': 938,\n",
       " 'related': 939,\n",
       " 'sight': 940,\n",
       " 'gags': 941,\n",
       " 'cause': 942,\n",
       " 'green': 943,\n",
       " 'grimace': 944,\n",
       " 'myer': 945,\n",
       " 'energy': 946,\n",
       " 'silliness': 947,\n",
       " 'eventually': 948,\n",
       " 'prevail': 949,\n",
       " 'simple': 950,\n",
       " 'seems': 951,\n",
       " 'working': 952,\n",
       " 'arbitrary': 953,\n",
       " 'remains': 954,\n",
       " 'seen': 955,\n",
       " 'whether': 956,\n",
       " 'statham': 957,\n",
       " 'move': 958,\n",
       " 'beyond': 959,\n",
       " 'crime': 960,\n",
       " 'land': 961,\n",
       " 'says': 962,\n",
       " 'killed': 963,\n",
       " 'my': 964,\n",
       " 'father': 965,\n",
       " 'art': 966,\n",
       " 're': 967,\n",
       " 'watching': 968,\n",
       " 'iceberg': 969,\n",
       " 'melt': 970,\n",
       " 'melts': 971,\n",
       " 'natured': 972,\n",
       " 'ensemble': 973,\n",
       " 'bumper': 974,\n",
       " 'cast': 975,\n",
       " 'quite': 976,\n",
       " 'ground': 977,\n",
       " 'kahn': 978,\n",
       " 'demanding': 979,\n",
       " 'progresses': 980,\n",
       " 'low': 981,\n",
       " 'key': 982,\n",
       " 'manner': 983,\n",
       " 'risks': 984,\n",
       " 'monotony': 985,\n",
       " 'started': 986,\n",
       " 'fell': 987,\n",
       " 'thanks': 988,\n",
       " 'largely': 989,\n",
       " 'developments': 990,\n",
       " 'processed': 991,\n",
       " 'minutes': 992,\n",
       " 'rest': 993,\n",
       " 'overexposed': 994,\n",
       " 'waste': 995,\n",
       " 'running': 996,\n",
       " 'screaming': 997,\n",
       " 'rather': 998,\n",
       " 'awful': 999,\n",
       " ...}"
      ]
     },
     "execution_count": 13,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "word_to_ix"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Vocab size = 17512\n"
     ]
    }
   ],
   "source": [
    "print(\"Vocab size = {}\".format(len(word_to_ix)))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [],
   "source": [
    "label_to_ix = { \"NEG\": 0, \"POS\": 1 }"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "17512"
      ]
     },
     "execution_count": 16,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "VOCAB_SIZE = len(word_to_ix)\n",
    "VOCAB_SIZE"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "2"
      ]
     },
     "execution_count": 17,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "NUM_LABELS = len(label_to_ix)\n",
    "NUM_LABELS"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 5. Making dataset iterable"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "1"
      ]
     },
     "execution_count": 18,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "batch_size = 10\n",
    "n_iters = 1000\n",
    "num_epochs = n_iters/(len(x_train) / batch_size)\n",
    "num_epochs = int(num_epochs)\n",
    "num_epochs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[('feeble comedy', 'NEG'),\n",
       " ('no doubt the star and everyone else involved had their hearts in the right place',\n",
       "  'POS'),\n",
       " ('you can drive right by it without noticing anything special save for few comic turns intended and otherwise',\n",
       "  'NEG'),\n",
       " ('an instance of an old dog not only learning but inventing remarkable new trick',\n",
       "  'POS'),\n",
       " ('although shot with little style skins is heartfelt and achingly real',\n",
       "  'POS')]"
      ]
     },
     "execution_count": 19,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "## iterable datasets\n",
    "train_data=list(zip(x_train,y_train))\n",
    "train_data[0:5]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[('the shackles', 'NEG'),\n",
       " ('it never plays as dramatic even when dramatic things happen to people',\n",
       "  'NEG'),\n",
       " ('an engaging formulaic sports drama that carries charge of genuine excitement',\n",
       "  'POS'),\n",
       " ('smart sassy interpretation of the oscar wilde play', 'POS'),\n",
       " ('ca n t remember the last time saw an audience laugh so much during movie but there s only one problem it s supposed to be drama',\n",
       "  'NEG')]"
      ]
     },
     "execution_count": 20,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "valid_data=list(zip(x_valid,y_valid))\n",
    "valid_data[0:5]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 7. Model - LSTM Classifier"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [],
   "source": [
    "EMBEDDING_DIM = 30\n",
    "HIDDEN_DIM = 50\n",
    "NUM_LAYERS = 1\n",
    "DROPOUT = 0"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [],
   "source": [
    "class LSTMClassifier(nn.Module):\n",
    "\n",
    "    def __init__(self, embedding_dim, hidden_dim, num_layers, vocab_size, label_size, dropout):\n",
    "        super(LSTMClassifier, self).__init__()\n",
    "        self.hidden_dim = hidden_dim\n",
    "        self.num_layers = num_layers\n",
    "        self.word_embeddings = nn.Embedding(vocab_size, embedding_dim)\n",
    "        self.lstm = nn.LSTM(embedding_dim, hidden_dim, num_layers, dropout=dropout, batch_first=False)\n",
    "        self.hidden2label = nn.Linear(hidden_dim, label_size)\n",
    "        self.hidden = self.init_hidden()\n",
    "\n",
    "    def init_hidden(self):\n",
    "        # the first is the hidden h\n",
    "        # the second is the cell  c\n",
    "        return (Variable(torch.zeros(self.num_layers, 1, self.hidden_dim)),\n",
    "                Variable(torch.zeros(self.num_layers, 1, self.hidden_dim)))\n",
    "\n",
    "    def forward(self, sentence):\n",
    "        embeds = self.word_embeddings(sentence)\n",
    "        x = embeds.view(len(sentence), 1, -1)\n",
    "        lstm_out, self.hidden = self.lstm(x, self.hidden)\n",
    "        y  = self.hidden2label(lstm_out[-1])\n",
    "        log_probs = F.log_softmax(y)\n",
    "        return log_probs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [],
   "source": [
    "model = LSTMClassifier(embedding_dim=EMBEDDING_DIM,\n",
    "                            hidden_dim=HIDDEN_DIM,\n",
    "                            num_layers=NUM_LAYERS,\n",
    "                            vocab_size=VOCAB_SIZE,\n",
    "                            label_size=NUM_LABELS,\n",
    "                            dropout=DROPOUT)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "LSTMClassifier (\n",
       "  (word_embeddings): Embedding(17512, 30)\n",
       "  (lstm): LSTM(30, 50)\n",
       "  (hidden2label): Linear (50 -> 2)\n",
       ")"
      ]
     },
     "execution_count": 24,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [],
   "source": [
    "def make_context_vector(seq, to_ix):\n",
    "    idxs = [to_ix[w] for w in seq.split()]\n",
    "    tensor = torch.LongTensor(idxs)\n",
    "    return tensor"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [],
   "source": [
    "def make_target(label, label_to_idx):\n",
    "    return torch.LongTensor([label_to_idx[label]])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 8. Loss Function and Optimizer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [],
   "source": [
    "loss_function = nn.CrossEntropyLoss()\n",
    "learning_rate = 0.01\n",
    "optimizer = optim.Adam(model.parameters(),lr = learning_rate)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'you can drive right by it without noticing anything special save for few comic turns intended and otherwise'"
      ]
     },
     "execution_count": 28,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "sample=train_data[2][0]\n",
    "sample"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "\n",
       "  20\n",
       " 129\n",
       " 130\n",
       " 131\n",
       " 132\n",
       " 133\n",
       "  68\n",
       " 134\n",
       "   4\n",
       " 135\n",
       "  68\n",
       " 136\n",
       "  97\n",
       " 137\n",
       "[torch.LongTensor of size 14]"
      ]
     },
     "execution_count": 29,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "make_context_vector(train_data[10][0],word_to_ix)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "Variable containing:\n",
       " 16\n",
       " 17\n",
       " 18\n",
       " 14\n",
       " 19\n",
       " 20\n",
       " 21\n",
       " 22\n",
       " 23\n",
       " 24\n",
       " 25\n",
       " 26\n",
       " 27\n",
       " 28\n",
       " 29\n",
       " 30\n",
       "  6\n",
       " 31\n",
       "[torch.LongTensor of size 18]"
      ]
     },
     "execution_count": 30,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "sample_context=Variable(make_context_vector(sample,word_to_ix))\n",
    "sample_context"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "Variable containing:\n",
       "-0.7626 -0.6282\n",
       "[torch.FloatTensor of size 1x2]"
      ]
     },
     "execution_count": 31,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "out=model(sample_context)\n",
    "out"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Iterations: 500. Loss: 0.3723790645599365. Accuracy: 54.74794841735053\n",
      "Iterations: 1000. Loss: 1.08137047290802. Accuracy: 56.15474794841735\n",
      "Iterations: 1500. Loss: 1.151354432106018. Accuracy: 56.389214536928485\n",
      "Iterations: 2000. Loss: 0.8430496454238892. Accuracy: 50.70339976553341\n",
      "Iterations: 2500. Loss: 0.742389976978302. Accuracy: 56.56506447831184\n",
      "Iterations: 3000. Loss: 0.6001772284507751. Accuracy: 55.74443141852286\n",
      "Iterations: 3500. Loss: 0.4139164984226227. Accuracy: 56.09613130128957\n",
      "Iterations: 4000. Loss: 1.0805284976959229. Accuracy: 54.51348182883939\n",
      "Iterations: 4500. Loss: 0.8681607246398926. Accuracy: 49.941383352872215\n",
      "Iterations: 5000. Loss: 1.1741716861724854. Accuracy: 54.161781946072686\n",
      "Iterations: 5500. Loss: 0.5108356475830078. Accuracy: 54.8651817116061\n",
      "Iterations: 6000. Loss: 0.28323712944984436. Accuracy: 53.458382180539274\n",
      "Iterations: 6500. Loss: 0.9901161193847656. Accuracy: 56.858147713950764\n"
     ]
    }
   ],
   "source": [
    "iter = 0\n",
    "for epoch in range(num_epochs):\n",
    "    for (sent,label) in train_data:\n",
    "        # Step 1 - clear the gradients\n",
    "        model.zero_grad()\n",
    "        optimizer.zero_grad()\n",
    "        model.hidden = model.init_hidden()\n",
    "    \n",
    "        ## Avoid breaking for empty input\n",
    "        try:\n",
    "            ## Step 2- Prepare input and label\n",
    "            context_vec = Variable(make_context_vector(sent, word_to_ix))\n",
    "            target = Variable(make_target(label, label_to_ix)) \n",
    "            # Step 3 - Run forward pass\n",
    "            output = model(context_vec)  \n",
    "            # Step 4 - Compute loss, gradients, update parameters\n",
    "            loss = loss_function(output, target)\n",
    "            loss.backward()\n",
    "            optimizer.step()\n",
    "        except:\n",
    "            pass\n",
    "        iter+=1      \n",
    "        ## Calculate final accuracy\n",
    "        if iter % 500 ==0:\n",
    "            correct = 0\n",
    "            total = 0\n",
    "            for (sent,label) in valid_data:\n",
    "                context_vec = Variable(make_context_vector(sent, word_to_ix))\n",
    "                target = Variable(make_target(label, label_to_ix))\n",
    "                output = model(context_vec)\n",
    "                _,predicted = torch.max(output.data,1)\n",
    "                total += target.size(0)\n",
    "                correct += (predicted == make_target(label, label_to_ix)).sum()\n",
    "            accuracy = 100 * correct/total\n",
    "            print('Iterations: {}. Loss: {}. Accuracy: {}'.format(iter,loss.data[0],accuracy))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "-------------------- INPUT ------------------------------\n",
      "TRUE LABEL = POS\n",
      "SENTENCE = smart sassy interpretation of the oscar wilde play\n",
      "-------------------- PREDICTION ------------------------------\n",
      "PRED = 0\n",
      "PRED = NEG\n",
      "PROBS = Variable containing:\n",
      " 0.6039  0.3961\n",
      "[torch.FloatTensor of size 1x2]\n",
      "\n"
     ]
    }
   ],
   "source": [
    "n=3\n",
    "bow_vec = Variable(make_context_vector(valid_data[n][0], word_to_ix))\n",
    "print(\"-\"*20 + \" INPUT \"+\"-\"*30)\n",
    "print(\"TRUE LABEL = {}\".format(valid_data[n][1]))\n",
    "print(\"SENTENCE = {}\".format(valid_data[n][0]))\n",
    "print(\"-\"*20 + \" PREDICTION \"+\"-\"*30)\n",
    "log_probs = model(bow_vec)\n",
    "_,predicted = torch.max(log_probs.data,1)\n",
    "print(\"PRED = {}\".format(predicted[0]))\n",
    "print(\"PRED = {}\".format(list(label_to_ix.keys())[list(label_to_ix.values()).index(predicted[0])]))\n",
    "##print(\"LOG_PROB = {}\".format(log_probs))\n",
    "print(\"PROBS = {}\".format(F.softmax(log_probs)))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Loading Pre-trained vectors"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "metadata": {},
   "outputs": [],
   "source": [
    "from gensim.models import KeyedVectors"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "GoogleNews-vectors-negative300.bin wiki-news-300d-1M-subword.vec\r\n",
      "glove.42B.300d.txt                 wiki-news-300d-1M.vec\r\n",
      "lid.176.ftz\r\n"
     ]
    }
   ],
   "source": [
    "!ls ../../../vectors"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "metadata": {},
   "outputs": [],
   "source": [
    "w2v = KeyedVectors.load_word2vec_format('~/repos/vectors/GoogleNews-vectors-negative300.bin', binary = True)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Build weights initialization matrix"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "metadata": {},
   "outputs": [],
   "source": [
    "W2V_DIM = 300\n",
    "## standard deviation to use\n",
    "sd = 1/np.sqrt(W2V_DIM)\n",
    "## Random initialization\n",
    "weights = np.random.normal(0, scale=sd, size=[VOCAB_SIZE, W2V_DIM])\n",
    "weights = weights.astype(np.float32)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([-0.03308766,  0.02778587, -0.05535707,  0.01274751, -0.01436458,\n",
       "       -0.00800677, -0.00284676,  0.08017665,  0.06320183, -0.08630445,\n",
       "        0.02757211,  0.02853842, -0.06298611, -0.03727243, -0.04372744,\n",
       "        0.01708458,  0.03430955,  0.0535158 ,  0.01962308, -0.04440845,\n",
       "       -0.06047032,  0.08077332,  0.01939448,  0.08300257, -0.03951006,\n",
       "        0.03059672,  0.12212086,  0.02846081, -0.01342562, -0.04188478,\n",
       "        0.16933852, -0.0456421 , -0.10265084,  0.00022967, -0.0214686 ,\n",
       "       -0.02636538, -0.07641384, -0.00283593, -0.01508848, -0.07175078,\n",
       "       -0.00822397,  0.04603168,  0.01206001, -0.03778317,  0.04014652,\n",
       "        0.0456126 ,  0.01643336, -0.01612621,  0.15607595,  0.01549839], dtype=float32)"
      ]
     },
     "execution_count": 38,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "weights[1][0:50]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "metadata": {},
   "outputs": [],
   "source": [
    "for word in word_to_ix:\n",
    "    id = word_to_ix.get(word,None)\n",
    "    if id is not None:\n",
    "        try:\n",
    "            weights[id]=w2v.wv.word_vec(word)\n",
    "        except:\n",
    "            weights[id]=np.random.normal(0, scale=sd, size=[1, W2V_DIM]) ## If word not present, initialize randomly"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'feeble': 0,\n",
       " 'comedy': 1,\n",
       " 'no': 2,\n",
       " 'doubt': 3,\n",
       " 'the': 4,\n",
       " 'star': 5,\n",
       " 'and': 6,\n",
       " 'everyone': 7,\n",
       " 'else': 8,\n",
       " 'involved': 9,\n",
       " 'had': 10,\n",
       " 'their': 11,\n",
       " 'hearts': 12,\n",
       " 'in': 13,\n",
       " 'right': 14,\n",
       " 'place': 15,\n",
       " 'you': 16,\n",
       " 'can': 17,\n",
       " 'drive': 18,\n",
       " 'by': 19,\n",
       " 'it': 20,\n",
       " 'without': 21,\n",
       " 'noticing': 22,\n",
       " 'anything': 23,\n",
       " 'special': 24,\n",
       " 'save': 25,\n",
       " 'for': 26,\n",
       " 'few': 27,\n",
       " 'comic': 28,\n",
       " 'turns': 29,\n",
       " 'intended': 30,\n",
       " 'otherwise': 31,\n",
       " 'an': 32,\n",
       " 'instance': 33,\n",
       " 'of': 34,\n",
       " 'old': 35,\n",
       " 'dog': 36,\n",
       " 'not': 37,\n",
       " 'only': 38,\n",
       " 'learning': 39,\n",
       " 'but': 40,\n",
       " 'inventing': 41,\n",
       " 'remarkable': 42,\n",
       " 'new': 43,\n",
       " 'trick': 44,\n",
       " 'although': 45,\n",
       " 'shot': 46,\n",
       " 'with': 47,\n",
       " 'little': 48,\n",
       " 'style': 49,\n",
       " 'skins': 50,\n",
       " 'is': 51,\n",
       " 'heartfelt': 52,\n",
       " 'achingly': 53,\n",
       " 'real': 54,\n",
       " 'fantastic': 55,\n",
       " 'premise': 56,\n",
       " 'anchors': 57,\n",
       " 'this': 58,\n",
       " 'movie': 59,\n",
       " 'what': 60,\n",
       " 'needs': 61,\n",
       " 'either': 62,\n",
       " 'more': 63,\n",
       " 'rigid': 64,\n",
       " 'blair': 65,\n",
       " 'witch': 66,\n",
       " 'commitment': 67,\n",
       " 'to': 68,\n",
       " 'its': 69,\n",
       " 'mockumentary': 70,\n",
       " 'format': 71,\n",
       " 'or': 72,\n",
       " 'straightforward': 73,\n",
       " 'dramatic': 74,\n",
       " 'treatment': 75,\n",
       " 'all': 76,\n",
       " 'grandiosity': 77,\n",
       " 'that': 78,\n",
       " 'implies': 79,\n",
       " 'wobbly': 80,\n",
       " 'senegalese': 81,\n",
       " 'updating': 82,\n",
       " 'carmen': 83,\n",
       " 'which': 84,\n",
       " 'best': 85,\n",
       " 'stunning': 86,\n",
       " 'turn': 87,\n",
       " 'djeinaba': 88,\n",
       " 'diop': 89,\n",
       " 'gai': 90,\n",
       " 'evokes': 91,\n",
       " 'flash': 92,\n",
       " 'double': 93,\n",
       " 'cross': 94,\n",
       " 'made': 95,\n",
       " 'mamet': 96,\n",
       " 's': 97,\n",
       " 'house': 98,\n",
       " 'games': 99,\n",
       " 'last': 100,\n",
       " 'fall': 101,\n",
       " 'heist': 102,\n",
       " 'so': 103,\n",
       " 'much': 104,\n",
       " 'fun': 105,\n",
       " 'film': 106,\n",
       " 'while': 107,\n",
       " 'exactly': 108,\n",
       " 'assured': 109,\n",
       " 'execution': 110,\n",
       " 'notable': 111,\n",
       " 'sheer': 112,\n",
       " 'audacity': 113,\n",
       " 'openness': 114,\n",
       " 'flat': 115,\n",
       " 'run': 116,\n",
       " 'at': 117,\n",
       " 'hip': 118,\n",
       " 'hop': 119,\n",
       " 'tootsie': 120,\n",
       " 'poorly': 121,\n",
       " 'paced': 122,\n",
       " 'could': 123,\n",
       " 'fit': 124,\n",
       " 'pootie': 125,\n",
       " 'tang': 126,\n",
       " 'between': 127,\n",
       " 'punchlines': 128,\n",
       " 'collapses': 129,\n",
       " 'when': 130,\n",
       " 'mr': 131,\n",
       " 'taylor': 132,\n",
       " 'tries': 133,\n",
       " 'shift': 134,\n",
       " 'tone': 135,\n",
       " 'thriller': 136,\n",
       " 'rush': 137,\n",
       " 'as': 138,\n",
       " 'tricky': 139,\n",
       " 'satisfying': 140,\n",
       " 'any': 141,\n",
       " 'david': 142,\n",
       " 'airless': 143,\n",
       " 'cinematic': 144,\n",
       " 'shell': 145,\n",
       " 'have': 146,\n",
       " 'been': 147,\n",
       " 'something': 148,\n",
       " 'two': 149,\n",
       " 'things': 150,\n",
       " 'drag': 151,\n",
       " 'down': 152,\n",
       " 'mediocrity': 153,\n",
       " 'director': 154,\n",
       " 'clare': 155,\n",
       " 'peploe': 156,\n",
       " 'misunderstanding': 157,\n",
       " 'marivaux': 158,\n",
       " 'rhythms': 159,\n",
       " 'mira': 160,\n",
       " 'sorvino': 161,\n",
       " 'limitations': 162,\n",
       " 'classical': 163,\n",
       " 'actress': 164,\n",
       " 'music': 165,\n",
       " 'makes': 166,\n",
       " 'nice': 167,\n",
       " 'album': 168,\n",
       " 'food': 169,\n",
       " 'enticing': 170,\n",
       " 'italy': 171,\n",
       " 'beckons': 172,\n",
       " 'us': 173,\n",
       " 'may': 174,\n",
       " 'be': 175,\n",
       " 'captivated': 176,\n",
       " 'was': 177,\n",
       " 'moods': 178,\n",
       " 'subtly': 179,\n",
       " 'transformed': 180,\n",
       " 'still': 181,\n",
       " 'wonder': 182,\n",
       " 'why': 183,\n",
       " 'paul': 184,\n",
       " 'thomas': 185,\n",
       " 'anderson': 186,\n",
       " 'ever': 187,\n",
       " 'inclination': 188,\n",
       " 'make': 189,\n",
       " 'most': 190,\n",
       " 'sincere': 191,\n",
       " 'artful': 192,\n",
       " 'adam': 193,\n",
       " 'sandler': 194,\n",
       " 'will': 195,\n",
       " 'probably': 196,\n",
       " 'appear': 197,\n",
       " 'remake': 198,\n",
       " 'pale': 199,\n",
       " 'imitation': 200,\n",
       " 'filmmakers': 201,\n",
       " 'dana': 202,\n",
       " 'janklowicz': 203,\n",
       " 'mann': 204,\n",
       " 'amir': 205,\n",
       " 'area': 206,\n",
       " 'headed': 207,\n",
       " 'east': 208,\n",
       " 'far': 209,\n",
       " 'retelling': 210,\n",
       " 'historically': 211,\n",
       " 'significant': 212,\n",
       " 'personal': 213,\n",
       " 'episode': 214,\n",
       " 'detailing': 215,\n",
       " 'how': 216,\n",
       " 'one': 217,\n",
       " 'international': 218,\n",
       " 'city': 219,\n",
       " 'welcomed': 220,\n",
       " 'tens': 221,\n",
       " 'thousands': 222,\n",
       " 'german': 223,\n",
       " 'jewish': 224,\n",
       " 'refugees': 225,\n",
       " 'world': 226,\n",
       " 'democracie': 227,\n",
       " 'bites': 228,\n",
       " 'hard': 229,\n",
       " 'troupe': 230,\n",
       " 'broken': 231,\n",
       " 'lizard': 232,\n",
       " 'first': 233,\n",
       " 'very': 234,\n",
       " 'funny': 235,\n",
       " 'too': 236,\n",
       " 'concerned': 237,\n",
       " 'giving': 238,\n",
       " 'plot': 239,\n",
       " 'shanghai': 240,\n",
       " 'ghetto': 241,\n",
       " 'roman': 242,\n",
       " 'polanski': 243,\n",
       " 'pianist': 244,\n",
       " 'compassionate': 245,\n",
       " 'spirit': 246,\n",
       " 'soars': 247,\n",
       " 'every': 248,\n",
       " 'bit': 249,\n",
       " 'high': 250,\n",
       " 'includes': 251,\n",
       " 'obvious': 252,\n",
       " 'padding': 253,\n",
       " 'both': 254,\n",
       " 'grant': 255,\n",
       " 'n': 256,\n",
       " 't': 257,\n",
       " 'cary': 258,\n",
       " 'bullock': 259,\n",
       " 'katherine': 260,\n",
       " 'benigni': 261,\n",
       " 'presents': 262,\n",
       " 'himself': 263,\n",
       " 'boy': 264,\n",
       " 'puppet': 265,\n",
       " 'pinocchio': 266,\n",
       " 'complete': 267,\n",
       " 'receding': 268,\n",
       " 'hairline': 269,\n",
       " 'weathered': 270,\n",
       " 'countenance': 271,\n",
       " 'american': 272,\n",
       " 'breckin': 273,\n",
       " 'meyer': 274,\n",
       " 'ridiculously': 275,\n",
       " 'inappropriate': 276,\n",
       " 'valley': 277,\n",
       " 'voice': 278,\n",
       " 'fresnadillo': 279,\n",
       " 'has': 280,\n",
       " 'serious': 281,\n",
       " 'say': 282,\n",
       " 'about': 283,\n",
       " 'ways': 284,\n",
       " 'extravagant': 285,\n",
       " 'chance': 286,\n",
       " 'distort': 287,\n",
       " 'our': 288,\n",
       " 'perspective': 289,\n",
       " 'throw': 290,\n",
       " 'off': 291,\n",
       " 'path': 292,\n",
       " 'good': 293,\n",
       " 'sense': 294,\n",
       " 'keenly': 295,\n",
       " 'observed': 296,\n",
       " 'refreshingly': 297,\n",
       " 'natural': 298,\n",
       " 'swimming': 299,\n",
       " 'gets': 300,\n",
       " 'details': 301,\n",
       " 'from': 302,\n",
       " 'promenade': 303,\n",
       " 'barely': 304,\n",
       " 'clad': 305,\n",
       " 'bodies': 306,\n",
       " 'myrtle': 307,\n",
       " 'beach': 308,\n",
       " 'c': 309,\n",
       " 'adrenaline': 310,\n",
       " 'jolt': 311,\n",
       " 'sudden': 312,\n",
       " 'lunch': 313,\n",
       " 'diner': 314,\n",
       " 'auteur': 315,\n",
       " 'ear': 316,\n",
       " 'way': 317,\n",
       " 'fears': 318,\n",
       " 'slights': 319,\n",
       " 'are': 320,\n",
       " 'telegraphed': 321,\n",
       " 'blithe': 322,\n",
       " 'exchanges': 323,\n",
       " 'gives': 324,\n",
       " 'lingering': 325,\n",
       " 'tug': 326,\n",
       " 'suffice': 327,\n",
       " 'after': 328,\n",
       " 'seeing': 329,\n",
       " 'imax': 330,\n",
       " 'form': 331,\n",
       " 'll': 332,\n",
       " 'acquainted': 333,\n",
       " 'tiniest': 334,\n",
       " 'tom': 335,\n",
       " 'hanks': 336,\n",
       " 'face': 337,\n",
       " 'than': 338,\n",
       " 'his': 339,\n",
       " 'wife': 340,\n",
       " 'dull': 341,\n",
       " 'somnambulant': 342,\n",
       " 'exercise': 343,\n",
       " 'pretension': 344,\n",
       " 'whose': 345,\n",
       " 'pervasive': 346,\n",
       " 'quiet': 347,\n",
       " 'frequent': 348,\n",
       " 'outbursts': 349,\n",
       " 'violence': 350,\n",
       " 'noise': 351,\n",
       " 'gangster': 352,\n",
       " 'solid': 353,\n",
       " 'fare': 354,\n",
       " 'adults': 355,\n",
       " 'efforts': 356,\n",
       " 'kline': 357,\n",
       " 'lend': 358,\n",
       " 'some': 359,\n",
       " 'dignity': 360,\n",
       " 'dumb': 361,\n",
       " 'story': 362,\n",
       " 'naught': 363,\n",
       " 'there': 364,\n",
       " 'movies': 365,\n",
       " 'hit': 366,\n",
       " 'scene': 367,\n",
       " 'know': 368,\n",
       " 'going': 369,\n",
       " 'trip': 370,\n",
       " 'scherfig': 371,\n",
       " 'light': 372,\n",
       " 'hearted': 373,\n",
       " 'profile': 374,\n",
       " 'emotional': 375,\n",
       " 'desperation': 376,\n",
       " 'honest': 377,\n",
       " 'delightfully': 378,\n",
       " 'cheeky': 379,\n",
       " 'mattei': 380,\n",
       " 'underdeveloped': 381,\n",
       " 'effort': 382,\n",
       " 'here': 383,\n",
       " 'nothing': 384,\n",
       " 'convenient': 385,\n",
       " 'conveyor': 386,\n",
       " 'belt': 387,\n",
       " 'brooding': 388,\n",
       " 'personalities': 389,\n",
       " 'parade': 390,\n",
       " 'if': 391,\n",
       " 'they': 392,\n",
       " 'were': 393,\n",
       " 'coming': 394,\n",
       " 'back': 395,\n",
       " 'stock': 396,\n",
       " 'character': 397,\n",
       " 'camp': 398,\n",
       " 'drowsy': 399,\n",
       " 'drama': 400,\n",
       " 'infatuated': 401,\n",
       " 'own': 402,\n",
       " 'pretentious': 403,\n",
       " 'self': 404,\n",
       " 'examination': 405,\n",
       " 'zings': 406,\n",
       " 'through': 407,\n",
       " 'originality': 408,\n",
       " 'humour': 409,\n",
       " 'pathos': 410,\n",
       " 'masterfully': 411,\n",
       " 'calibrated': 412,\n",
       " 'psychological': 413,\n",
       " 'thrives': 414,\n",
       " 'on': 415,\n",
       " 'taut': 416,\n",
       " 'performances': 417,\n",
       " 'creepy': 418,\n",
       " 'atmosphere': 419,\n",
       " 'even': 420,\n",
       " 'screenplay': 421,\n",
       " 'falls': 422,\n",
       " 'somewhat': 423,\n",
       " 'short': 424,\n",
       " 'charming': 425,\n",
       " 'clashing': 426,\n",
       " 'cultures': 427,\n",
       " 'mother': 428,\n",
       " 'daughter': 429,\n",
       " 'relationship': 430,\n",
       " 'moral': 431,\n",
       " 'shrapnel': 432,\n",
       " 'mental': 433,\n",
       " 'shellshock': 434,\n",
       " 'linger': 435,\n",
       " 'long': 436,\n",
       " 'ended': 437,\n",
       " 'incredibly': 438,\n",
       " 'thoughtful': 439,\n",
       " 'deeply': 440,\n",
       " 'meditative': 441,\n",
       " 'picture': 442,\n",
       " 'neatly': 443,\n",
       " 'effectively': 444,\n",
       " 'captures': 445,\n",
       " 'debilitating': 446,\n",
       " 'grief': 447,\n",
       " 'felt': 448,\n",
       " 'immediate': 449,\n",
       " 'aftermath': 450,\n",
       " 'terrorist': 451,\n",
       " 'attacks': 452,\n",
       " 'films': 453,\n",
       " 'year': 454,\n",
       " 'exquisite': 455,\n",
       " 'acting': 456,\n",
       " 'inventive': 457,\n",
       " 'mesmerizing': 458,\n",
       " 'many': 459,\n",
       " 'inimitable': 460,\n",
       " 'scenes': 461,\n",
       " 'tenderness': 462,\n",
       " 'loss': 463,\n",
       " 'discontent': 464,\n",
       " 'yearning': 465,\n",
       " 'baaaaaaaaad': 466,\n",
       " 'van': 467,\n",
       " 'wilder': 468,\n",
       " 'worst': 469,\n",
       " 'national': 470,\n",
       " 'lampoon': 471,\n",
       " 'being': 472,\n",
       " 'generation': 473,\n",
       " 'animal': 474,\n",
       " 'think': 475,\n",
       " 'twice': 476,\n",
       " 'might': 477,\n",
       " 'inside': 478,\n",
       " 'each': 479,\n",
       " 'trailer': 480,\n",
       " 'park': 481,\n",
       " 'past': 482,\n",
       " 'chiefly': 483,\n",
       " 'inspires': 484,\n",
       " 'faster': 485,\n",
       " 'enough': 486,\n",
       " 'gun': 487,\n",
       " 'battles': 488,\n",
       " 'throwaway': 489,\n",
       " 'humor': 490,\n",
       " 'cover': 491,\n",
       " 'up': 492,\n",
       " 'yawning': 493,\n",
       " 'chasm': 494,\n",
       " 'where': 495,\n",
       " 'should': 496,\n",
       " 'same': 497,\n",
       " 'mistake': 498,\n",
       " 'industry': 499,\n",
       " 'criticizes': 500,\n",
       " 'becoming': 501,\n",
       " 'slick': 502,\n",
       " 'watered': 503,\n",
       " 'almost': 504,\n",
       " 'loses': 505,\n",
       " 'love': 506,\n",
       " 'biggest': 507,\n",
       " 'problem': 508,\n",
       " 'lrb': 509,\n",
       " 'other': 510,\n",
       " 'sluggish': 511,\n",
       " 'pace': 512,\n",
       " 'rrb': 513,\n",
       " 'we': 514,\n",
       " 'never': 515,\n",
       " 'really': 516,\n",
       " 'see': 517,\n",
       " 'her': 518,\n",
       " 'esther': 519,\n",
       " 'blossom': 520,\n",
       " 'though': 521,\n",
       " 'talent': 522,\n",
       " 'supposed': 523,\n",
       " 'growing': 524,\n",
       " 'better': 525,\n",
       " 'understand': 526,\n",
       " 'did': 527,\n",
       " 'connect': 528,\n",
       " 'me': 529,\n",
       " 'would': 530,\n",
       " 'require': 531,\n",
       " 'another': 532,\n",
       " 'viewing': 533,\n",
       " 'wo': 534,\n",
       " 'sitting': 535,\n",
       " 'again': 536,\n",
       " 'itself': 537,\n",
       " 'commentary': 538,\n",
       " 'sheridan': 539,\n",
       " 'painfully': 540,\n",
       " 'bad': 541,\n",
       " 'fourth': 542,\n",
       " 'rate': 543,\n",
       " 'jim': 544,\n",
       " 'carrey': 545,\n",
       " 'who': 546,\n",
       " 'does': 547,\n",
       " 'difference': 548,\n",
       " 'just': 549,\n",
       " 'plain': 550,\n",
       " 'carvey': 551,\n",
       " 'characters': 552,\n",
       " 'overplayed': 553,\n",
       " 'exaggerated': 554,\n",
       " 'then': 555,\n",
       " 'subtlety': 556,\n",
       " 'trademark': 557,\n",
       " 'master': 558,\n",
       " 'disguise': 559,\n",
       " 'under': 560,\n",
       " 'category': 561,\n",
       " 'sketch': 562,\n",
       " 'saturday': 563,\n",
       " 'night': 564,\n",
       " 'live': 565,\n",
       " 'john': 566,\n",
       " 'carlen': 567,\n",
       " 'script': 568,\n",
       " 'full': 569,\n",
       " 'unhappy': 570,\n",
       " 'dimensional': 571,\n",
       " 'compelling': 572,\n",
       " 'small': 573,\n",
       " 'big': 574,\n",
       " 'heart': 575,\n",
       " 'age': 576,\n",
       " 'arduous': 577,\n",
       " 'journey': 578,\n",
       " 'sensitive': 579,\n",
       " 'young': 580,\n",
       " 'girl': 581,\n",
       " 'series': 582,\n",
       " 'foster': 583,\n",
       " 'homes': 584,\n",
       " 'fierce': 585,\n",
       " 'struggle': 586,\n",
       " 'pull': 587,\n",
       " 'free': 588,\n",
       " 'dangerous': 589,\n",
       " 'domineering': 590,\n",
       " 'hold': 591,\n",
       " 'over': 592,\n",
       " 'despite': 593,\n",
       " 'flaws': 594,\n",
       " 'crazy': 595,\n",
       " 'hell': 596,\n",
       " 'marks': 597,\n",
       " 'encouraging': 598,\n",
       " 'direction': 599,\n",
       " 'la': 600,\n",
       " 'salle': 601,\n",
       " 'shallow': 602,\n",
       " 'welcome': 603,\n",
       " 'accept': 604,\n",
       " 'trials': 605,\n",
       " 'henry': 606,\n",
       " 'kissinger': 607,\n",
       " 'faithful': 608,\n",
       " 'portraiture': 609,\n",
       " 'argue': 610,\n",
       " 'debate': 611,\n",
       " 'joins': 612,\n",
       " 'necessary': 613,\n",
       " 'timely': 614,\n",
       " 'apart': 615,\n",
       " 'considerable': 616,\n",
       " 'achievement': 617,\n",
       " 'goyer': 618,\n",
       " 'loose': 619,\n",
       " 'unaccountable': 620,\n",
       " 'technically': 621,\n",
       " 'sophisticated': 622,\n",
       " 'runs': 623,\n",
       " 'longer': 624,\n",
       " 'muccino': 625,\n",
       " 'notice': 626,\n",
       " 'ends': 627,\n",
       " 'ca': 628,\n",
       " 'tear': 629,\n",
       " 'away': 630,\n",
       " 'smooth': 631,\n",
       " 'professional': 632,\n",
       " 'tsai': 633,\n",
       " 'ploughing': 634,\n",
       " 'furrow': 635,\n",
       " 'once': 636,\n",
       " 'often': 637,\n",
       " 'end': 638,\n",
       " 'bogs': 639,\n",
       " 'insignificance': 640,\n",
       " 'saying': 641,\n",
       " 'kennedy': 642,\n",
       " 'assassination': 643,\n",
       " 'revealing': 644,\n",
       " 'pathology': 645,\n",
       " 'pretends': 646,\n",
       " 'investigate': 647,\n",
       " 'essentially': 648,\n",
       " 'fleetingly': 649,\n",
       " 'interesting': 650,\n",
       " 'actors': 651,\n",
       " 'moments': 652,\n",
       " 'passion': 653,\n",
       " 'irwin': 654,\n",
       " 'earnest': 655,\n",
       " 'resist': 656,\n",
       " 'pleas': 657,\n",
       " 'spare': 658,\n",
       " 'wildlife': 659,\n",
       " 'respect': 660,\n",
       " 'environs': 661,\n",
       " 'rarely': 662,\n",
       " 'indeed': 663,\n",
       " 'such': 664,\n",
       " 'wattage': 665,\n",
       " 'brainpower': 666,\n",
       " 'coupled': 667,\n",
       " 'pitch': 668,\n",
       " 'perfect': 669,\n",
       " 'unfakable': 670,\n",
       " 'cinema': 671,\n",
       " 'antwone': 672,\n",
       " 'fisher': 673,\n",
       " 'certainly': 674,\n",
       " 'making': 675,\n",
       " 'care': 676,\n",
       " 'protagonist': 677,\n",
       " 'celebrate': 678,\n",
       " 'victories': 679,\n",
       " 'exceptions': 680,\n",
       " 'stoops': 681,\n",
       " 'cheap': 682,\n",
       " 'manipulation': 683,\n",
       " 'corny': 684,\n",
       " 'conventions': 685,\n",
       " 'do': 686,\n",
       " 'intriguing': 687,\n",
       " 'twist': 688,\n",
       " 'french': 689,\n",
       " 'genre': 690,\n",
       " 'princess': 691,\n",
       " 'seem': 692,\n",
       " 'smug': 693,\n",
       " 'cartoonish': 694,\n",
       " 'comes': 695,\n",
       " 'alive': 696,\n",
       " 'poor': 697,\n",
       " 'hermocrates': 698,\n",
       " 'leontine': 699,\n",
       " 'pathetically': 700,\n",
       " 'compare': 701,\n",
       " 'notes': 702,\n",
       " 'budding': 703,\n",
       " 'amours': 704,\n",
       " 'aside': 705,\n",
       " 'fact': 706,\n",
       " 'idiotically': 707,\n",
       " 'uses': 708,\n",
       " 'website': 709,\n",
       " 'feardotcom': 710,\n",
       " 'com': 711,\n",
       " 'improperly': 712,\n",
       " 'hammy': 713,\n",
       " 'performance': 714,\n",
       " 'stephen': 715,\n",
       " 'rea': 716,\n",
       " 'added': 717,\n",
       " 'disdain': 718,\n",
       " 'nearly': 719,\n",
       " 'impossible': 720,\n",
       " 'look': 721,\n",
       " 'perfectly': 722,\n",
       " 'competent': 723,\n",
       " 'imaginative': 724,\n",
       " 'lacks': 725,\n",
       " 'lilo': 726,\n",
       " 'stitch': 727,\n",
       " 'spades': 728,\n",
       " 'charisma': 729,\n",
       " 'overwrought': 730,\n",
       " 'ending': 731,\n",
       " 'works': 732,\n",
       " 'well': 733,\n",
       " 'because': 734,\n",
       " 'people': 735,\n",
       " 'want': 736,\n",
       " 'ol': 737,\n",
       " 'ball': 738,\n",
       " 'chain': 739,\n",
       " 'those': 740,\n",
       " 'waterboy': 741,\n",
       " 'major': 742,\n",
       " 'windtalkers': 743,\n",
       " 'bulk': 744,\n",
       " 'centers': 745,\n",
       " 'wrong': 746,\n",
       " 'hilarity': 747,\n",
       " 'simply': 748,\n",
       " 'recommend': 749,\n",
       " 'determination': 750,\n",
       " 'pinochet': 751,\n",
       " 'victims': 752,\n",
       " 'seek': 753,\n",
       " 'justice': 754,\n",
       " 'heartbreaking': 755,\n",
       " 'testimony': 756,\n",
       " 'spoken': 757,\n",
       " 'directly': 758,\n",
       " 'into': 759,\n",
       " 'patricio': 760,\n",
       " 'guzman': 761,\n",
       " 'camera': 762,\n",
       " 'pack': 763,\n",
       " 'powerful': 764,\n",
       " 'wallop': 765,\n",
       " 'road': 766,\n",
       " 'tight': 767,\n",
       " 'pants': 768,\n",
       " 'tits': 769,\n",
       " 'stupid': 770,\n",
       " 'ramsay': 771,\n",
       " 'morton': 772,\n",
       " 'fill': 773,\n",
       " 'study': 774,\n",
       " 'poetic': 775,\n",
       " 'force': 776,\n",
       " 'buoyant': 777,\n",
       " 'feeling': 778,\n",
       " 'consider': 779,\n",
       " 'review': 780,\n",
       " 'life': 781,\n",
       " 'affirming': 782,\n",
       " 'turned': 783,\n",
       " 'out': 784,\n",
       " 'hours': 785,\n",
       " 'unfocused': 786,\n",
       " 'excruciatingly': 787,\n",
       " 'tedious': 788,\n",
       " 'half': 789,\n",
       " 'hour': 790,\n",
       " 'starts': 791,\n",
       " 'water': 792,\n",
       " 'torture': 793,\n",
       " 'appealing': 794,\n",
       " 'narc': 795,\n",
       " 'menace': 796,\n",
       " 'exception': 797,\n",
       " 'blighter': 798,\n",
       " 'particular': 799,\n",
       " 'south': 800,\n",
       " 'london': 801,\n",
       " 'housing': 802,\n",
       " 'project': 803,\n",
       " 'digs': 804,\n",
       " 'dysfunction': 805,\n",
       " 'like': 806,\n",
       " 'comforting': 807,\n",
       " 'jar': 808,\n",
       " 'marmite': 809,\n",
       " 'slathered': 810,\n",
       " 'crackers': 811,\n",
       " 'served': 812,\n",
       " 'feast': 813,\n",
       " 'bleakness': 814,\n",
       " 'instead': 815,\n",
       " 'go': 816,\n",
       " 'rent': 817,\n",
       " 'shakes': 818,\n",
       " 'clown': 819,\n",
       " 'funnier': 820,\n",
       " 'similar': 821,\n",
       " 'theme': 822,\n",
       " 'equally': 823,\n",
       " 'great': 824,\n",
       " 'robin': 825,\n",
       " 'williams': 826,\n",
       " 'spy': 827,\n",
       " 'kids': 828,\n",
       " 'island': 829,\n",
       " 'lost': 830,\n",
       " 'dreams': 831,\n",
       " 'franchise': 832,\n",
       " 'establishes': 833,\n",
       " 'durable': 834,\n",
       " 'part': 835,\n",
       " 'landscape': 836,\n",
       " 'james': 837,\n",
       " 'bond': 838,\n",
       " 'action': 839,\n",
       " 'icon': 840,\n",
       " 'decommissioned': 841,\n",
       " 'your': 842,\n",
       " 'subject': 843,\n",
       " 'illusion': 844,\n",
       " 'versus': 845,\n",
       " 'reality': 846,\n",
       " 'least': 847,\n",
       " 'passably': 848,\n",
       " 'testament': 849,\n",
       " 'divine': 850,\n",
       " 'calling': 851,\n",
       " 'education': 852,\n",
       " 'demonstration': 853,\n",
       " 'painstaking': 854,\n",
       " 'process': 855,\n",
       " 'imparting': 856,\n",
       " 'knowledge': 857,\n",
       " 'remarkably': 858,\n",
       " 'alluring': 859,\n",
       " 'set': 860,\n",
       " 'constrictive': 861,\n",
       " 'eisenhower': 862,\n",
       " 'era': 863,\n",
       " 'suburban': 864,\n",
       " 'woman': 865,\n",
       " 'shatters': 866,\n",
       " 'cheery': 867,\n",
       " 'tranquil': 868,\n",
       " 'averse': 869,\n",
       " 'usually': 870,\n",
       " 'am': 871,\n",
       " 'feel': 872,\n",
       " 'follow': 873,\n",
       " 'dream': 874,\n",
       " 'hollywood': 875,\n",
       " 'fantasies': 876,\n",
       " 'got': 877,\n",
       " 'concert': 878,\n",
       " 'footage': 879,\n",
       " 'stirring': 880,\n",
       " 'recording': 881,\n",
       " 'sessions': 882,\n",
       " 'striking': 883,\n",
       " 'blow': 884,\n",
       " 'artistic': 885,\n",
       " 'integrity': 886,\n",
       " 'quality': 887,\n",
       " 'band': 888,\n",
       " 'pick': 889,\n",
       " 'admirers': 890,\n",
       " 'rare': 891,\n",
       " 'documentary': 892,\n",
       " 'incorporates': 893,\n",
       " 'human': 894,\n",
       " 'experience': 895,\n",
       " 'conflict': 896,\n",
       " 'tears': 897,\n",
       " 'surprise': 898,\n",
       " 'transcends': 899,\n",
       " 'normal': 900,\n",
       " 'divisions': 901,\n",
       " 'fiction': 902,\n",
       " 'nonfiction': 903,\n",
       " 'payne': 904,\n",
       " 'created': 905,\n",
       " 'beautiful': 906,\n",
       " 'canvas': 907,\n",
       " 'nicholson': 908,\n",
       " 'proves': 909,\n",
       " 'he': 910,\n",
       " 'brush': 911,\n",
       " 'business': 912,\n",
       " 'robert': 913,\n",
       " 'burke': 914,\n",
       " 'monster': 915,\n",
       " 'horns': 916,\n",
       " 'steals': 917,\n",
       " 'show': 918,\n",
       " 'family': 919,\n",
       " 'sour': 920,\n",
       " 'immortals': 921,\n",
       " 'saved': 922,\n",
       " 'merely': 923,\n",
       " 'cool': 924,\n",
       " 'basic': 925,\n",
       " 'credible': 926,\n",
       " 'compassion': 927,\n",
       " 'reminiscent': 928,\n",
       " 'unforgiven': 929,\n",
       " 'also': 930,\n",
       " 'utilized': 931,\n",
       " 'scintillating': 932,\n",
       " 'draw': 933,\n",
       " 'sparse': 934,\n",
       " 'dialogue': 935,\n",
       " 'shamelessly': 936,\n",
       " 'resorting': 937,\n",
       " 'pee': 938,\n",
       " 'related': 939,\n",
       " 'sight': 940,\n",
       " 'gags': 941,\n",
       " 'cause': 942,\n",
       " 'green': 943,\n",
       " 'grimace': 944,\n",
       " 'myer': 945,\n",
       " 'energy': 946,\n",
       " 'silliness': 947,\n",
       " 'eventually': 948,\n",
       " 'prevail': 949,\n",
       " 'simple': 950,\n",
       " 'seems': 951,\n",
       " 'working': 952,\n",
       " 'arbitrary': 953,\n",
       " 'remains': 954,\n",
       " 'seen': 955,\n",
       " 'whether': 956,\n",
       " 'statham': 957,\n",
       " 'move': 958,\n",
       " 'beyond': 959,\n",
       " 'crime': 960,\n",
       " 'land': 961,\n",
       " 'says': 962,\n",
       " 'killed': 963,\n",
       " 'my': 964,\n",
       " 'father': 965,\n",
       " 'art': 966,\n",
       " 're': 967,\n",
       " 'watching': 968,\n",
       " 'iceberg': 969,\n",
       " 'melt': 970,\n",
       " 'melts': 971,\n",
       " 'natured': 972,\n",
       " 'ensemble': 973,\n",
       " 'bumper': 974,\n",
       " 'cast': 975,\n",
       " 'quite': 976,\n",
       " 'ground': 977,\n",
       " 'kahn': 978,\n",
       " 'demanding': 979,\n",
       " 'progresses': 980,\n",
       " 'low': 981,\n",
       " 'key': 982,\n",
       " 'manner': 983,\n",
       " 'risks': 984,\n",
       " 'monotony': 985,\n",
       " 'started': 986,\n",
       " 'fell': 987,\n",
       " 'thanks': 988,\n",
       " 'largely': 989,\n",
       " 'developments': 990,\n",
       " 'processed': 991,\n",
       " 'minutes': 992,\n",
       " 'rest': 993,\n",
       " 'overexposed': 994,\n",
       " 'waste': 995,\n",
       " 'running': 996,\n",
       " 'screaming': 997,\n",
       " 'rather': 998,\n",
       " 'awful': 999,\n",
       " ...}"
      ]
     },
     "execution_count": 40,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "word_to_ix"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([ 0.1640625 ,  0.18847656,  0.14160156, -0.02941895,  0.02087402,\n",
       "        0.13769531, -0.0168457 , -0.32617188,  0.07519531, -0.05200195,\n",
       "        0.11816406,  0.09179688,  0.06689453, -0.04614258, -0.04321289,\n",
       "        0.38476562,  0.0213623 , -0.09423828,  0.05712891,  0.18066406,\n",
       "       -0.08740234,  0.3359375 , -0.078125  , -0.07861328, -0.02111816,\n",
       "       -0.28320312,  0.08740234,  0.1796875 ,  0.11083984,  0.0480957 ,\n",
       "       -0.00469971,  0.03857422,  0.01940918,  0.15332031,  0.07714844,\n",
       "        0.01574707,  0.21875   ,  0.16113281, -0.14257812,  0.12695312,\n",
       "        0.04736328, -0.48242188,  0.10302734,  0.11816406,  0.24316406,\n",
       "       -0.00631714, -0.04858398,  0.05395508,  0.31835938,  0.16113281], dtype=float32)"
      ]
     },
     "execution_count": 41,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "weights[5][0:50]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 42,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([ 0.1640625 ,  0.18847656,  0.14160156, -0.02941895,  0.02087402,\n",
       "        0.13769531, -0.0168457 , -0.32617188,  0.07519531, -0.05200195,\n",
       "        0.11816406,  0.09179688,  0.06689453, -0.04614258, -0.04321289,\n",
       "        0.38476562,  0.0213623 , -0.09423828,  0.05712891,  0.18066406,\n",
       "       -0.08740234,  0.3359375 , -0.078125  , -0.07861328, -0.02111816,\n",
       "       -0.28320312,  0.08740234,  0.1796875 ,  0.11083984,  0.0480957 ,\n",
       "       -0.00469971,  0.03857422,  0.01940918,  0.15332031,  0.07714844,\n",
       "        0.01574707,  0.21875   ,  0.16113281, -0.14257812,  0.12695312,\n",
       "        0.04736328, -0.48242188,  0.10302734,  0.11816406,  0.24316406,\n",
       "       -0.00631714, -0.04858398,  0.05395508,  0.31835938,  0.16113281], dtype=float32)"
      ]
     },
     "execution_count": 42,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "w2v.wv.word_vec(\"star\")[0:50]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 43,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([  1.36718750e-01,   1.48437500e-01,   1.14746094e-01,\n",
       "         6.98242188e-02,  -1.66992188e-01,  -6.28662109e-03,\n",
       "         3.51562500e-01,  -7.91015625e-02,  -8.44726562e-02,\n",
       "         2.19726562e-02,  -9.47265625e-02,  -9.47265625e-02,\n",
       "        -1.95312500e-03,  -4.12597656e-02,  -1.39648438e-01,\n",
       "         1.77734375e-01,   6.34765625e-02,   2.27539062e-01,\n",
       "         8.20312500e-02,  -8.83789062e-02,  -1.88476562e-01,\n",
       "        -3.36914062e-02,   9.57031250e-02,   8.78906250e-02,\n",
       "         1.09863281e-01,   9.71679688e-02,   7.96318054e-05,\n",
       "         7.56835938e-02,   7.32421875e-03,  -1.23535156e-01,\n",
       "         1.03027344e-01,   1.65039062e-01,  -3.83300781e-02,\n",
       "        -2.24609375e-01,   8.98437500e-02,   6.93359375e-02,\n",
       "         1.96289062e-01,   7.76367188e-02,  -3.97949219e-02,\n",
       "         9.96093750e-02,   4.17480469e-02,  -1.53320312e-01,\n",
       "         1.66015625e-01,   1.56250000e-02,  -1.99218750e-01,\n",
       "        -1.76757812e-01,   8.39843750e-02,   2.46582031e-02,\n",
       "        -8.00781250e-02,  -1.26953125e-01], dtype=float32)"
      ]
     },
     "execution_count": 43,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "w2v.wv.word_vec(\"want\")[0:50]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 44,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "736"
      ]
     },
     "execution_count": 44,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "word_to_ix['want']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 45,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([  1.36718750e-01,   1.48437500e-01,   1.14746094e-01,\n",
       "         6.98242188e-02,  -1.66992188e-01,  -6.28662109e-03,\n",
       "         3.51562500e-01,  -7.91015625e-02,  -8.44726562e-02,\n",
       "         2.19726562e-02,  -9.47265625e-02,  -9.47265625e-02,\n",
       "        -1.95312500e-03,  -4.12597656e-02,  -1.39648438e-01,\n",
       "         1.77734375e-01,   6.34765625e-02,   2.27539062e-01,\n",
       "         8.20312500e-02,  -8.83789062e-02,  -1.88476562e-01,\n",
       "        -3.36914062e-02,   9.57031250e-02,   8.78906250e-02,\n",
       "         1.09863281e-01,   9.71679688e-02,   7.96318054e-05,\n",
       "         7.56835938e-02,   7.32421875e-03,  -1.23535156e-01,\n",
       "         1.03027344e-01,   1.65039062e-01,  -3.83300781e-02,\n",
       "        -2.24609375e-01,   8.98437500e-02,   6.93359375e-02,\n",
       "         1.96289062e-01,   7.76367188e-02,  -3.97949219e-02,\n",
       "         9.96093750e-02,   4.17480469e-02,  -1.53320312e-01,\n",
       "         1.66015625e-01,   1.56250000e-02,  -1.99218750e-01,\n",
       "        -1.76757812e-01,   8.39843750e-02,   2.46582031e-02,\n",
       "        -8.00781250e-02,  -1.26953125e-01], dtype=float32)"
      ]
     },
     "execution_count": 45,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "weights[736][0:50]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## LSTM Classifier with Word2vec"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 46,
   "metadata": {},
   "outputs": [],
   "source": [
    "W2V_DIM = 300\n",
    "HIDDEN_DIM = 50\n",
    "NUM_LAYERS = 1\n",
    "DROPOUT = 0"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 47,
   "metadata": {},
   "outputs": [],
   "source": [
    "class LSTMClassifierW2vec(nn.Module):\n",
    "\n",
    "    def __init__(self, embedding_dim, hidden_dim, num_layers, vocab_size, label_size, pre_trained_weights, dropout):\n",
    "        super(LSTMClassifierW2vec, self).__init__()\n",
    "        self.hidden_dim = hidden_dim\n",
    "        self.num_layers = num_layers\n",
    "        self.word_embeddings = nn.Embedding(vocab_size, embedding_dim)\n",
    "        #print(pre_trained_weights[5][0:10])\n",
    "        self.word_embeddings.weight.data=torch.Tensor(pre_trained_weights)\n",
    "        self.lstm = nn.LSTM(embedding_dim, hidden_dim, num_layers, batch_first=False,dropout=dropout,bidirectional=False)\n",
    "        self.hidden2label = nn.Linear(hidden_dim, label_size)\n",
    "        self.hidden = self.init_hidden()\n",
    "\n",
    "    def init_hidden(self):\n",
    "        # the first is the hidden h\n",
    "        # the second is the cell  c\n",
    "        return (Variable(torch.zeros(self.num_layers, 1, self.hidden_dim)),\n",
    "                Variable(torch.zeros(self.num_layers, 1, self.hidden_dim)))\n",
    "\n",
    "    def forward(self, sentence):\n",
    "        embeds = self.word_embeddings(sentence)\n",
    "        x = embeds.view(len(sentence), 1, -1)\n",
    "        lstm_out, self.hidden = self.lstm(x, self.hidden)\n",
    "        y  = self.hidden2label(lstm_out[-1])\n",
    "        log_probs = F.log_softmax(y)\n",
    "        return log_probs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 48,
   "metadata": {},
   "outputs": [],
   "source": [
    "model_w2v = LSTMClassifierW2vec(embedding_dim=W2V_DIM,\n",
    "                            hidden_dim=HIDDEN_DIM,\n",
    "                            num_layers=NUM_LAYERS,\n",
    "                            vocab_size=VOCAB_SIZE,\n",
    "                            label_size=NUM_LABELS,\n",
    "                            pre_trained_weights = weights,\n",
    "                            dropout = DROPOUT)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 49,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "LSTMClassifierW2vec (\n",
       "  (word_embeddings): Embedding(17512, 300)\n",
       "  (lstm): LSTM(300, 50)\n",
       "  (hidden2label): Linear (50 -> 2)\n",
       ")"
      ]
     },
     "execution_count": 49,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "model_w2v"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 50,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "Variable containing:\n",
       " 5\n",
       "[torch.LongTensor of size 1]"
      ]
     },
     "execution_count": 50,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "## Testing if weights were copies\n",
    "msg = \"star\"\n",
    "sample_context=Variable(make_context_vector(msg,word_to_ix))\n",
    "sample_context"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 51,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([ 0.1640625 ,  0.18847656,  0.14160156, -0.02941895,  0.02087402,\n",
       "        0.13769531, -0.0168457 , -0.32617188,  0.07519531, -0.05200195,\n",
       "        0.11816406,  0.09179688,  0.06689453, -0.04614258, -0.04321289,\n",
       "        0.38476562,  0.0213623 , -0.09423828,  0.05712891,  0.18066406,\n",
       "       -0.08740234,  0.3359375 , -0.078125  , -0.07861328, -0.02111816,\n",
       "       -0.28320312,  0.08740234,  0.1796875 ,  0.11083984,  0.0480957 ,\n",
       "       -0.00469971,  0.03857422,  0.01940918,  0.15332031,  0.07714844,\n",
       "        0.01574707,  0.21875   ,  0.16113281, -0.14257812,  0.12695312,\n",
       "        0.04736328, -0.48242188,  0.10302734,  0.11816406,  0.24316406,\n",
       "       -0.00631714, -0.04858398,  0.05395508,  0.31835938,  0.16113281], dtype=float32)"
      ]
     },
     "execution_count": 51,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "w2v.wv.word_vec(\"star\")[0:50]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 52,
   "metadata": {},
   "outputs": [],
   "source": [
    "loss_function_w2v = nn.CrossEntropyLoss()\n",
    "learning_rate_w2v = 0.001\n",
    "optimizer_w2v = optim.Adam(model_w2v.parameters(),lr = learning_rate_w2v)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 53,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'you can drive right by it without noticing anything special save for few comic turns intended and otherwise'"
      ]
     },
     "execution_count": 53,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "sample=train_data[2][0]\n",
    "sample"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 54,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "Variable containing:\n",
       " 16\n",
       " 17\n",
       " 18\n",
       " 14\n",
       " 19\n",
       " 20\n",
       " 21\n",
       " 22\n",
       " 23\n",
       " 24\n",
       " 25\n",
       " 26\n",
       " 27\n",
       " 28\n",
       " 29\n",
       " 30\n",
       "  6\n",
       " 31\n",
       "[torch.LongTensor of size 18]"
      ]
     },
     "execution_count": 54,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "sample_context=Variable(make_context_vector(sample,word_to_ix))\n",
    "sample_context"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 55,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "Variable containing:\n",
       "-0.6449 -0.7439\n",
       "[torch.FloatTensor of size 1x2]"
      ]
     },
     "execution_count": 55,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "out=model_w2v(sample_context)\n",
    "out"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Iterations: 500. Loss: 1.5224510431289673. Accuracy: 67.87807737397421\n",
      "Iterations: 1000. Loss: 0.5993427038192749. Accuracy: 70.1641266119578\n",
      "Iterations: 1500. Loss: 2.0562186241149902. Accuracy: 67.81946072684643\n",
      "Iterations: 2000. Loss: 0.1832445114850998. Accuracy: 70.80890973036342\n",
      "Iterations: 2500. Loss: 0.1929742395877838. Accuracy: 71.16060961313013\n",
      "Iterations: 3000. Loss: 0.3004419803619385. Accuracy: 69.69519343493552\n",
      "Iterations: 3500. Loss: 0.7613804340362549. Accuracy: 70.63305978898008\n",
      "Iterations: 4000. Loss: 0.22596986591815948. Accuracy: 69.34349355216881\n",
      "Iterations: 4500. Loss: 0.275987446308136. Accuracy: 64.88862837045721\n",
      "Iterations: 5000. Loss: 0.12038353085517883. Accuracy: 67.99531066822978\n",
      "Iterations: 5500. Loss: 0.21046432852745056. Accuracy: 70.69167643610785\n",
      "Iterations: 6000. Loss: 0.0739918053150177. Accuracy: 70.22274325908558\n"
     ]
    }
   ],
   "source": [
    "iter = 0\n",
    "for epoch in range(num_epochs):\n",
    "    for (sent,label) in train_data:\n",
    "        # Step 1 - clear the gradients\n",
    "        model_w2v.zero_grad()\n",
    "        optimizer_w2v.zero_grad()\n",
    "        model_w2v.hidden = model_w2v.init_hidden()\n",
    "    \n",
    "        ## Avoid breaking for empty input\n",
    "        try:\n",
    "            ## Step 2- Prepare input and label\n",
    "            context_vec = Variable(make_context_vector(sent, word_to_ix))\n",
    "            target = Variable(make_target(label, label_to_ix)) \n",
    "            # Step 3 - Run forward pass\n",
    "            output = model_w2v(context_vec)  \n",
    "            # Step 4 - Compute loss, gradients, update parameters\n",
    "            loss = loss_function_w2v(output, target)\n",
    "            loss.backward()\n",
    "            optimizer_w2v.step()\n",
    "        except:\n",
    "            pass\n",
    "        iter+=1      \n",
    "        ## Calculate final accuracy\n",
    "        if iter % 500 ==0:\n",
    "            correct = 0\n",
    "            total = 0\n",
    "            for (sent,label) in valid_data:\n",
    "                context_vec = Variable(make_context_vector(sent, word_to_ix))\n",
    "                target = Variable(make_target(label, label_to_ix))\n",
    "                output = model_w2v(context_vec)\n",
    "                _,predicted = torch.max(output.data,1)\n",
    "                total += target.size(0)\n",
    "                correct += (predicted == make_target(label, label_to_ix)).sum()\n",
    "            accuracy = 100 * correct/total\n",
    "            print('Iterations: {}. Loss: {}. Accuracy: {}'.format(iter,loss.data[0],accuracy))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "n=3\n",
    "context_vec = Variable(make_context_vector(valid_data[n][0], word_to_ix))\n",
    "print(\"-\"*20 + \" INPUT \"+\"-\"*30)\n",
    "print(\"TRUE LABEL = {}\".format(valid_data[n][1]))\n",
    "print(\"SENTENCE = {}\".format(valid_data[n][0]))\n",
    "print(\"-\"*20 + \" PREDICTION \"+\"-\"*30)\n",
    "log_probs = model_w2v(context_vec)\n",
    "_,predicted = torch.max(log_probs.data,1)\n",
    "print(\"PRED = {}\".format(predicted[0]))\n",
    "print(\"PRED = {}\".format(list(label_to_ix.keys())[list(label_to_ix.values()).index(predicted[0])]))\n",
    "##print(\"LOG_PROB = {}\".format(log_probs))\n",
    "print(\"PROBS = {}\".format(F.softmax(log_probs)))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
